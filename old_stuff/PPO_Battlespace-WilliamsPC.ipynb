{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in e:\\anaconda\\lib\\site-packages (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in e:\\anaconda\\lib\\site-packages (1.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gym in e:\\anaconda\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in e:\\anaconda\\lib\\site-packages (from gym) (1.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in e:\\anaconda\\lib\\site-packages (from gym) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pygame\n",
    "%pip install numpy\n",
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "from gym import spaces\n",
    "import os\n",
    "import sys\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "DISP_WIDTH = 1000\n",
    "DISP_HEIGHT = 1000\n",
    "FPS = pygame.time.Clock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Default Config ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG_DICT = {\n",
    "    'time_step': 0.1, # hours per time step\n",
    "    'plane_speed': 500, # mph\n",
    "    'bullet_speed': 700, # mph\n",
    "    'max_time': 10, # hours the epoch can last\n",
    "    'show_viz': False, # show the pygame animation\n",
    "    'step_turn': 30, # degrees to turn per step\n",
    "    'hit_base_reward': 1000, # reward for shooting enemy base\n",
    "    'hit_plane_reward': 1000, # reward for shooting enemy plane\n",
    "    'miss_punishment': -5, # punishment for missing a shot\n",
    "    'too_long_punishment': -1, # punishment for taking too long to end the game\n",
    "    'closer_to_base_reward': 0, # reward for getting closer to enemy base\n",
    "    'closer_to_plane_reward': 0, # reward for getting closer to enemy plane\n",
    "    'turn_to_base_reward': 0, # reward for turning towards the enemy base\n",
    "    'turn_to_plane_reward': 0 # reward for turning towards the enemy plane\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Helper Functions ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(p1, p0):\n",
    "    return math.degrees(math.atan2(p1[1]-p0[1],p1[0]-p0[0]))\n",
    "\n",
    "def dist(p1,p0):\n",
    "    return math.sqrt((p1[0]-p0[0])**2 + (p1[1]-p0[1])**2)\n",
    "\n",
    "def blitRotate(image, pos, originPos, angle):\n",
    "\n",
    "    # offset from pivot to center\n",
    "    image_rect = image.get_rect(topleft = (pos[0] - originPos[0], pos[1]-originPos[1]))\n",
    "    offset_center_to_pivot = pygame.math.Vector2(pos) - image_rect.center\n",
    "    \n",
    "    # roatated offset from pivot to center\n",
    "    rotated_offset = offset_center_to_pivot.rotate(-angle)\n",
    "\n",
    "    # rotated image center\n",
    "    rotated_image_center = (pos[0] - rotated_offset.x, pos[1] - rotated_offset.y)\n",
    "\n",
    "    # get a rotated image\n",
    "    rotated_image = pygame.transform.rotate(image, angle)\n",
    "    rotated_image_rect = rotated_image.get_rect(center = rotated_image_center)\n",
    "\n",
    "    return rotated_image, rotated_image_rect\n",
    "\n",
    "def calc_new_xy(old_xy, speed, time, angle):\n",
    "    new_x = old_xy[0] + (speed*time*math.cos(-math.radians(angle)))\n",
    "    new_y = old_xy[1] + (speed*time*math.sin(-math.radians(angle)))\n",
    "    return (new_x, new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Plane Class ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 124,
     "status": "ok",
     "timestamp": 1654185833596,
     "user": {
      "displayName": "Shane Forry",
      "userId": "07873955147565922030"
     },
     "user_tz": 300
    },
    "id": "6UIhXw5Tn6Qf"
   },
   "outputs": [],
   "source": [
    "class Plane:\n",
    "    def __init__(self, team): \n",
    "        self.team = team\n",
    "        self.image = pygame.image.load(f\"Images/{team}_plane.png\")\n",
    "        self.w, self.h = self.image.get_size()\n",
    "        self.heading = 0\n",
    "        self.rect = self.image.get_rect()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        if self.team == 'red':\n",
    "            x = (DISP_WIDTH - self.w/2)/2 * random.random()\n",
    "            y = (DISP_HEIGHT - self.h/2) * random.random()\n",
    "            self.rect.center = (x, y)\n",
    "            self.heading = 90 * random.random() if random.random() < .5 else 90 * random.random() + 270\n",
    "            \n",
    "        else:\n",
    "            x = (DISP_WIDTH - self.w/2)/2 * random.random() + (DISP_WIDTH - self.w/2)/2\n",
    "            y = (DISP_HEIGHT - self.h/2) * random.random()\n",
    "            self.rect.center = (x, y)\n",
    "            self.heading = 180 * random.random() + 90\n",
    "        \n",
    "    def rotate(self, angle):\n",
    "        self.heading += angle\n",
    "\n",
    "    def set_heading(self, heading):\n",
    "        self.heading = heading\n",
    "\n",
    "    def forward(self, speed, time):\n",
    "        oldpos = self.rect.center\n",
    "        self.rect.center = calc_new_xy(oldpos, speed, time, self.heading)\n",
    "\n",
    "    def draw(self, surface):\n",
    "        image, rect = blitRotate(self.image, self.rect.center, (self.w/2, self.h/2), self.heading)\n",
    "        surface.blit(image, rect)\n",
    "\n",
    "    def update(self):\n",
    "        # Keep player on the screen\n",
    "        if self.rect.left < 0:\n",
    "            self.rect.left = 0\n",
    "        if self.rect.right > DISP_WIDTH:\n",
    "            self.rect.right = DISP_WIDTH\n",
    "        if self.rect.top <= 0:\n",
    "            self.rect.top = 0\n",
    "        if self.rect.bottom >= DISP_HEIGHT:\n",
    "            self.rect.bottom = DISP_HEIGHT\n",
    "\n",
    "    def get_pos(self):\n",
    "        return self.rect.center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------- Base Class ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    \n",
    "    def __init__(self, team):\n",
    "        self.team = team\n",
    "        self.image = pygame.image.load(f\"Images/{team}_base.png\")\n",
    "        self.w, self.h = self.image.get_size()\n",
    "        self.rect = self.image.get_rect()\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        if self.team == 'red':\n",
    "            x = (DISP_WIDTH - self.w/2)/2 * random.random()\n",
    "            y = (DISP_HEIGHT - self.h/2) * random.random()\n",
    "            self.rect.center = (x, y)\n",
    "        else:\n",
    "            x = (DISP_WIDTH - self.w/2)/2 * random.random() + (DISP_WIDTH - self.w/2)/2\n",
    "            y = (DISP_HEIGHT - self.h/2) * random.random()\n",
    "            self.rect.center = (x, y)\n",
    "\n",
    "    def draw(self, surface):\n",
    "        surface.blit(self.image, self.rect)\n",
    "            \n",
    "    def get_pos(self):\n",
    "        return self.rect.center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Bullet Class ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bullet(pygame.sprite.Sprite):\n",
    "    def __init__(self, x, y, angle, speed, fteam, oteam):\n",
    "        pygame.sprite.Sprite.__init__(self)\n",
    "        self.off_screen = False\n",
    "        self.image = pygame.Surface((8, 4), pygame.SRCALPHA)\n",
    "        self.fteam = fteam\n",
    "        self.color = RED if self.fteam == 'red' else BLUE\n",
    "        self.oteam = oteam\n",
    "        self.image.fill(self.color)\n",
    "        self.rect = self.image.get_rect(center=(x, y))\n",
    "        self.w, self.h = self.image.get_size()\n",
    "        self.heading = angle + (random.random() * 10 - 5) # +- 5 degrees for randomness\n",
    "        self.pos = (x, y)\n",
    "        self.speed = speed\n",
    "\n",
    "    def update(self, screen_width, screen_height, time):\n",
    "        oldpos = self.rect.center\n",
    "        self.rect.center = calc_new_xy(oldpos, self.speed, time, self.heading)\n",
    "        if self.rect.centerx > screen_width or self.rect.centerx < 0 or self.rect.centery > screen_height or self.rect.centery < 0:\n",
    "            return 'miss'\n",
    "        for plane in self.oteam['planes']:\n",
    "            if self.rect.colliderect(plane.rect):\n",
    "                return 'plane'\n",
    "        if self.rect.colliderect(self.oteam['base'].rect):\n",
    "            return 'base'\n",
    "        return 'none'\n",
    "\n",
    "    def draw(self, surface):\n",
    "        image, rect = blitRotate(self.image, self.rect.center, (self.w/2, self.h/2), self.heading)\n",
    "        surface.blit(image, rect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------- Battle Environment ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1662,
     "status": "ok",
     "timestamp": 1654185839436,
     "user": {
      "displayName": "Shane Forry",
      "userId": "07873955147565922030"
     },
     "user_tz": 300
    },
    "id": "tAEp-Jot_1qZ"
   },
   "outputs": [],
   "source": [
    "class BattleEnvironment(gym.Env):\n",
    "    def __init__(self, config: dict=DEFAULT_CONFIG_DICT):\n",
    "        super(BattleEnvironment, self).__init__()\n",
    "        self.width = DISP_WIDTH\n",
    "        self.height = DISP_HEIGHT\n",
    "        self.max_time = config['max_time']\n",
    "        high = np.array( # Observation space: fplane_pos_x, fplane_pos_y, fplane_angle, dist_obase, dist_oplane, rel_angle_obase, rel_angle_oplane\n",
    "            [\n",
    "                self.width,\n",
    "                self.height,\n",
    "                720,\n",
    "                math.sqrt(math.pow(self.width, 2) + math.pow(self.height, 2)),\n",
    "                math.sqrt(math.pow(self.width, 2) + math.pow(self.height, 2)),\n",
    "                720,\n",
    "                720,\n",
    "                self.max_time\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        # For the Agent, actions are turn left, turn right, turn to enemy, turn to target, go forward, or shoot\n",
    "        # For the Random choice Agent, the actions are to enemy, to target, shoot enemy, or shoot target\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.random_action_space = [0, 1, 4, 5]\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "        self.team = {}\n",
    "        self.team['red'] = {}\n",
    "        self.team['blue'] = {}\n",
    "        self.team['red']['base'] = Base('red')\n",
    "        self.team['blue']['base'] = Base('blue')\n",
    "        self.team['red']['planes'] = []\n",
    "        self.team['red']['planes'].append(Plane('red'))\n",
    "        self.team['blue']['planes'] = []\n",
    "        self.team['blue']['planes'].append(Plane('blue'))\n",
    "        self.team['red']['wins'] = 0\n",
    "        self.team['blue']['wins'] = 0\n",
    "        self.ties = 0\n",
    "        self.bullets = []\n",
    "        self.time_step = config['time_step']\n",
    "        self.speed = config['plane_speed']\n",
    "        self.bullet_speed = config['bullet_speed']\n",
    "        self.total_time = 0 # in hours\n",
    "        self.show = config['show_viz']\n",
    "        self.step_turn = config['step_turn']\n",
    "        self.hit_base_reward = config['hit_base_reward']\n",
    "        self.hit_plane_reward = config['hit_plane_reward']\n",
    "        self.miss_punishment = config['miss_punishment']\n",
    "        self.too_long_punishment = config['too_long_punishment']\n",
    "        self.closer_to_base_reward = config['closer_to_base_reward']\n",
    "        self.closer_to_plane_reward = config['closer_to_plane_reward']\n",
    "        self.turn_to_base_reward = config['turn_to_base_reward']\n",
    "        self.turn_to_plane_reward = config['turn_to_plane_reward']\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # This code will all have to be changed when adding multiple planes\n",
    "        fplane = self.team['red']['planes'][0]\n",
    "        oplane = self.team['blue']['planes'][0]\n",
    "        obase = self.team['red']['base']\n",
    "\n",
    "        fplane_pos = fplane.get_pos()\n",
    "        fplane_angle = fplane.heading\n",
    "        oplane_pos = oplane.get_pos()\n",
    "        obase_pos = obase.get_pos()\n",
    "\n",
    "        dist_oplane = dist(oplane_pos, fplane_pos)\n",
    "        dist_obase = dist(obase_pos, fplane_pos)\n",
    "\n",
    "        angle_to_oplane = get_angle(oplane_pos, fplane_pos)\n",
    "        angle_to_obase = get_angle(obase_pos, fplane_pos)\n",
    "        rel_angle_oplane = (angle_to_oplane - fplane_angle) % 360\n",
    "        rel_angle_obase = (angle_to_obase - fplane_angle) % 360\n",
    "        \n",
    "        self.observation = (fplane_pos[0], fplane_pos[1], fplane_angle, dist_obase, dist_oplane, rel_angle_obase, rel_angle_oplane, self.total_time)\n",
    "        return np.array(self.observation, dtype=np.float32)\n",
    "\n",
    "        \n",
    "    def reset(self): # return observation\n",
    "        self.done = False\n",
    "        self.winner = 'none'\n",
    "\n",
    "        self.team['red']['base'].reset()\n",
    "        self.team['blue']['base'].reset()\n",
    "\n",
    "        for plane in self.team['red']['planes']:\n",
    "            plane.reset()\n",
    "        for plane in self.team['blue']['planes']:\n",
    "            plane.reset()\n",
    "\n",
    "        self.total_time = 0\n",
    "        self.bullets = []\n",
    "\n",
    "        if self.show:\n",
    "            pygame.init()\n",
    "            self.display = pygame.display.set_mode((DISP_WIDTH, DISP_HEIGHT))\n",
    "            pygame.display.set_caption(\"Battlespace Simulator\")\n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action): # return observation, reward, done, info\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        # Check if over time, if so, end game in tie\n",
    "        self.total_time += self.time_step\n",
    "        if self.total_time >= self.max_time:\n",
    "            self.done = True\n",
    "            self.ties += 1\n",
    "            if self.show:\n",
    "                self.render()\n",
    "                print(\"Draw\")\n",
    "            return self._get_observation(), 0, self.done, {}\n",
    "\n",
    "        # Red turn\n",
    "        self.friendly = 'red'\n",
    "        self.opponent = 'blue'\n",
    "        reward += self._process_action(action, self.team[self.friendly], self.team[self.opponent])\n",
    "        \n",
    "        # Blue turn\n",
    "        self.friendly = 'blue'\n",
    "        self.opponent = 'red'\n",
    "        self._process_action(self.random_action_space[random.randint(0, 3)], self.team[self.friendly], self.team[self.opponent])        \n",
    "\n",
    "        # Check if bullets hit and move them\n",
    "        for bullet in self.bullets:\n",
    "            outcome = bullet.update(self.width, self.height, self.time_step)\n",
    "            if outcome == 'miss':\n",
    "                reward += self.miss_punishment\n",
    "                self.bullets.pop(self.bullets.index(bullet))\n",
    "            elif outcome == 'plane' or outcome == 'base': # If a bullet hit\n",
    "                self.winner = bullet.fteam\n",
    "                self.team[self.winner]['wins'] += 1\n",
    "                self.done = True\n",
    "                reward = reward + self.hit_base_reward if outcome == 'base' else reward + self.hit_plane_reward\n",
    "                if self.show:\n",
    "                    self.render()\n",
    "                    print(f\"{self.winner} wins\")\n",
    "                return self._get_observation(), reward, self.done, {}\n",
    "            \n",
    "        # Check if past half of max time and give punishment\n",
    "        if (self.total_time > self.max_time//2):\n",
    "            reward += self.too_long_punishment\n",
    "\n",
    "        # Continue game\n",
    "        if self.show:\n",
    "            self.render()\n",
    "        return self._get_observation(), reward, self.done, {}\n",
    "    \n",
    "    def _process_action(self, action, fteam, oteam): # friendly and opponent\n",
    "        reward = 0\n",
    "\n",
    "        fplane = fteam['planes'][0]\n",
    "        oplane = oteam['planes'][0]\n",
    "        obase = oteam['base']\n",
    "\n",
    "        fplane_pos = fplane.get_pos()\n",
    "        fplane_angle = fplane.heading\n",
    "        oplane_pos = oplane.get_pos()\n",
    "        obase_pos = obase.get_pos()\n",
    "\n",
    "        dist_oplane = dist(oplane_pos, fplane_pos)\n",
    "        dist_obase = dist(obase_pos, fplane_pos)\n",
    "\n",
    "        angle_to_oplane = get_angle(oplane_pos, fplane_pos)\n",
    "        angle_to_obase = get_angle(obase_pos, fplane_pos)\n",
    "        rel_angle_oplane = (angle_to_oplane - fplane_angle) % 360\n",
    "        rel_angle_obase = (angle_to_obase - fplane_angle) % 360\n",
    "\n",
    "        # --------------- FORWARDS ---------------\n",
    "        if action == 0: \n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "\n",
    "         # --------------- SHOOT ---------------\n",
    "        elif action == 1:\n",
    "            self.bullets.append(Bullet(fplane_pos[0], fplane_pos[1], fplane_angle, self.bullet_speed, self.friendly, oteam))\n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "        \n",
    "        # --------------- TURN RIGHT ---------------\n",
    "        elif action == 2:\n",
    "            fplane.rotate(-self.step_turn)\n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "\n",
    "        # ---------------- TURN LEFT ----------------\n",
    "        elif action == 3:\n",
    "            fplane.rotate(self.step_turn)\n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "\n",
    "        # ---------------- TURN TO OPLANE ----------------\n",
    "        elif action == 4:\n",
    "            if math.fabs(rel_angle_oplane) < self.step_turn: # within step_turn of base\n",
    "                fplane.set_heading(angle_to_oplane)\n",
    "\n",
    "            elif math.fabs(rel_angle_oplane) > 360 - self.step_turn: # within step_turn of base\n",
    "                fplane.set_heading(angle_to_oplane)\n",
    "\n",
    "            elif math.fabs(rel_angle_oplane) < 180: # turn right\n",
    "                fplane.rotate(-self.step_turn)\n",
    "\n",
    "            else: # turn left\n",
    "                fplane.rotate(self.step_turn)\n",
    "                \n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "\n",
    "        # ---------------- TURN TO OBASE ----------------\n",
    "        elif action == 5:\n",
    "            if math.fabs(rel_angle_obase) < self.step_turn: # within step_turn of base\n",
    "                fplane.set_heading(angle_to_obase)\n",
    "\n",
    "            elif math.fabs(rel_angle_obase) > 360 - self.step_turn: # within step_turn of base\n",
    "                fplane.set_heading(angle_to_obase)\n",
    "\n",
    "            elif math.fabs(rel_angle_obase) < 180: # turn right\n",
    "                fplane.rotate(-self.step_turn)\n",
    "\n",
    "            else: # turn left\n",
    "                fplane.rotate(self.step_turn)\n",
    "\n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "        \n",
    "        # ---------------- GIVE REWARDS IF CLOSER (DIST OR ANGLE) ----------------\n",
    "        new_fplane_pos = fplane.get_pos()\n",
    "        new_fplane_angle = fplane.heading\n",
    "        new_oplane_pos = oplane.get_pos()\n",
    "        new_obase_pos = obase.get_pos()\n",
    "\n",
    "        new_dist_oplane = dist(new_oplane_pos, new_fplane_pos)\n",
    "        new_dist_obase = dist(new_obase_pos, new_fplane_pos)\n",
    "\n",
    "        new_angle_to_oplane = get_angle(new_oplane_pos, new_fplane_pos)\n",
    "        new_angle_to_obase = get_angle(new_obase_pos, new_fplane_pos)\n",
    "        new_rel_angle_oplane = (new_angle_to_oplane - new_fplane_angle) % 360\n",
    "        new_rel_angle_obase = (new_angle_to_obase - new_fplane_angle) % 360\n",
    "\n",
    "        if new_dist_oplane < dist_oplane: # If got closer to enemy plane\n",
    "            reward += self.closer_to_plane_reward\n",
    "\n",
    "        if new_dist_obase < dist_obase: # If got closer to enemy base\n",
    "            reward += self.closer_to_base_reward\n",
    "\n",
    "        if math.fabs(new_rel_angle_oplane) < math.fabs(rel_angle_oplane): # If aiming closer to enemy plane\n",
    "            reward += self.turn_to_plane_reward\n",
    "\n",
    "        if math.fabs(new_rel_angle_obase) < math.fabs(rel_angle_obase): # If aiming closer to enemy base\n",
    "            reward += self.turn_to_base_reward\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def draw_shot(self, hit, friendly_pos, target_pos, team):\n",
    "        color = (0, 0, 0)\n",
    "        color = (255, 0, 0) if team == 'red' else (0, 0, 255)\n",
    "        if not hit: target_pos = (target_pos[0] + (random.random() * 2 - 1) * 100, target_pos[1] + (random.random() * 2 - 1) * 100)\n",
    "        self.shot_history.append((hit, friendly_pos, target_pos, color))\n",
    "    \n",
    "    def winner_screen(self):\n",
    "        if self.show:\n",
    "            font = pygame.font.Font('freesansbold.ttf', 32)\n",
    "            if self.winner != 'none':\n",
    "                text = font.render(f\"THE WINNER IS {self.winner.upper()}\", True, (0, 0, 0))\n",
    "                textRect = text.get_rect()\n",
    "                textRect.center = (DISP_WIDTH//2, DISP_HEIGHT//2)\n",
    "            else:\n",
    "                text = font.render(f\"THE GAME IS A TIE\", True, (0, 0, 0))\n",
    "                textRect = text.get_rect()\n",
    "                textRect.center = (DISP_WIDTH//2, DISP_HEIGHT//2)\n",
    "            self.display.blit(text, textRect)\n",
    "\n",
    "    def show_wins(self):\n",
    "        print(\"Wins by red:\", self.team['red']['wins'])\n",
    "        print(\"Wins by blue:\", self.team['blue']['wins'])\n",
    "        print(\"Tied games:\", self.ties)\n",
    "\n",
    "    def render(self):\n",
    "        if self.show: # Just to ensure it won't render if self.show == False\n",
    "            for event in pygame.event.get():\n",
    "                # Check for KEYDOWN event\n",
    "                if event.type == KEYDOWN:\n",
    "                    # If the Esc key is pressed, then exit the main loop\n",
    "                    if event.key == K_ESCAPE:\n",
    "                        pygame.quit()\n",
    "                        sys.exit()\n",
    "                        return\n",
    "                # Check for QUIT event. If QUIT, then set running to false.\n",
    "                elif event.type == QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "                    return\n",
    "                    \n",
    "            # Fill background\n",
    "            self.display.fill(WHITE)\n",
    "\n",
    "            # Draw bullets\n",
    "            for bullet in self.bullets:\n",
    "                bullet.draw(self.display)\n",
    "                    \n",
    "            # Draw bases\n",
    "            self.team['red']['base'].draw(self.display)\n",
    "            self.team['blue']['base'].draw(self.display)\n",
    "\n",
    "            # Draw planes\n",
    "            for plane in self.team['red']['planes']:\n",
    "                plane.update()\n",
    "                plane.draw(self.display)\n",
    "            for plane in self.team['blue']['planes']:\n",
    "                plane.update()\n",
    "                plane.draw(self.display)\n",
    "\n",
    "            # Winner Screen\n",
    "            if self.done:\n",
    "                font = pygame.font.Font('freesansbold.ttf', 32)\n",
    "                if self.winner != 'none':\n",
    "                    text = font.render(f\"THE WINNER IS {self.winner.upper()}\", True, (0, 0, 0))\n",
    "                    textRect = text.get_rect()\n",
    "                    textRect.center = (DISP_WIDTH//2, DISP_HEIGHT//2)\n",
    "                else:\n",
    "                    text = font.render(f\"THE GAME IS A TIE\", True, (0, 0, 0))\n",
    "                    textRect = text.get_rect()\n",
    "                    textRect.center = (DISP_WIDTH//2, DISP_HEIGHT//2)\n",
    "                self.display.blit(text, textRect)\n",
    "                pygame.display.update()\n",
    "                pygame.time.wait(3000)\n",
    "                pygame.quit()\n",
    "                return\n",
    "            \n",
    "            pygame.display.update()\n",
    "            FPS.tick(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Hyperparameter Tuning ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
      "Requirement already satisfied: torch==1.10.2+cu113 in e:\\anaconda\\lib\\site-packages (1.10.2+cu113)\n",
      "Requirement already satisfied: torchvision==0.11.3+cu113 in e:\\anaconda\\lib\\site-packages (0.11.3+cu113)\n",
      "Requirement already satisfied: torchaudio===0.10.2+cu113 in e:\\anaconda\\lib\\site-packages (0.10.2+cu113)\n",
      "Requirement already satisfied: typing-extensions in e:\\anaconda\\lib\\site-packages (from torch==1.10.2+cu113) (4.1.1)\n",
      "Requirement already satisfied: numpy in e:\\anaconda\\lib\\site-packages (from torchvision==0.11.3+cu113) (1.21.0)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in e:\\anaconda\\lib\\site-packages (from torchvision==0.11.3+cu113) (9.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: stable-baselines3[extra] in e:\\anaconda\\lib\\site-packages (1.5.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: optuna in e:\\anaconda\\lib\\site-packages (2.10.1)\n",
      "Requirement already satisfied: gym==0.21 in e:\\anaconda\\lib\\site-packages (from stable-baselines3[extra]) (0.21.0)\n",
      "Requirement already satisfied: numpy in e:\\anaconda\\lib\\site-packages (from stable-baselines3[extra]) (1.21.0)\n",
      "Requirement already satisfied: matplotlib in e:\\anaconda\\lib\\site-packages (from stable-baselines3[extra]) (3.5.1)\n",
      "Requirement already satisfied: cloudpickle in e:\\anaconda\\lib\\site-packages (from stable-baselines3[extra]) (2.0.0)\n",
      "Requirement already satisfied: pandas in e:\\anaconda\\lib\\site-packages (from stable-baselines3[extra]) (1.4.2)\n",
      "Requirement already satisfied: torch>=1.8.1 in e:\\anaconda\\lib\\site-packages (from stable-baselines3[extra]) (1.10.2+cu113)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in e:\\anaconda\\lib\\site-packages (from stable-baselines3[extra]) (0.4.2)\n",
      "Requirement already satisfied: ale-py~=0.7.4 in e:\\anaconda\\lib\\site-packages (from stable-baselines3[extra]) (0.7.5)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in e:\\anaconda\\lib\\site-packages (from stable-baselines3[extra]) (2.9.0)\n",
      "Requirement already satisfied: psutil in e:\\anaconda\\lib\\site-packages (from stable-baselines3[extra]) (5.8.0)\n",
      "Requirement already satisfied: opencv-python in e:\\anaconda\\lib\\site-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
      "Requirement already satisfied: pillow in e:\\anaconda\\lib\\site-packages (from stable-baselines3[extra]) (9.0.1)\n",
      "Requirement already satisfied: scipy!=1.4.0 in e:\\anaconda\\lib\\site-packages (from optuna) (1.7.3)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\anaconda\\lib\\site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda\\lib\\site-packages (from optuna) (4.64.0)\n",
      "Requirement already satisfied: alembic in e:\\anaconda\\lib\\site-packages (from optuna) (1.8.0)\n",
      "Requirement already satisfied: colorlog in e:\\anaconda\\lib\\site-packages (from optuna) (6.6.0)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in e:\\anaconda\\lib\\site-packages (from optuna) (0.8.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in e:\\anaconda\\lib\\site-packages (from optuna) (1.4.32)\n",
      "Requirement already satisfied: PyYAML in e:\\anaconda\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: cliff in e:\\anaconda\\lib\\site-packages (from optuna) (3.10.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in e:\\anaconda\\lib\\site-packages (from ale-py~=0.7.4->stable-baselines3[extra]) (4.11.3)\n",
      "Requirement already satisfied: importlib-resources in e:\\anaconda\\lib\\site-packages (from ale-py~=0.7.4->stable-baselines3[extra]) (5.7.1)\n",
      "Requirement already satisfied: click in e:\\anaconda\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (8.0.4)\n",
      "Requirement already satisfied: requests in e:\\anaconda\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.27.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in e:\\anaconda\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (0.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in e:\\anaconda\\lib\\site-packages (from importlib-metadata>=4.10.0->ale-py~=0.7.4->stable-baselines3[extra]) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\anaconda\\lib\\site-packages (from packaging>=20.0->optuna) (3.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in e:\\anaconda\\lib\\site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in e:\\anaconda\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (62.3.4)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in e:\\anaconda\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.19.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in e:\\anaconda\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in e:\\anaconda\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.42.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in e:\\anaconda\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.1.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in e:\\anaconda\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in e:\\anaconda\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in e:\\anaconda\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in e:\\anaconda\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in e:\\anaconda\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.33.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in e:\\anaconda\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in e:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in e:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in e:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in e:\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in e:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in e:\\anaconda\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.26.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in e:\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions in e:\\anaconda\\lib\\site-packages (from torch>=1.8.1->stable-baselines3[extra]) (4.1.1)\n",
      "Requirement already satisfied: Mako in e:\\anaconda\\lib\\site-packages (from alembic->optuna) (1.2.0)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\lib\\site-packages (from click->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (0.4.4)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in e:\\anaconda\\lib\\site-packages (from cliff->optuna) (2.4.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in e:\\anaconda\\lib\\site-packages (from cliff->optuna) (5.9.0)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in e:\\anaconda\\lib\\site-packages (from cliff->optuna) (3.5.0)\n",
      "Requirement already satisfied: autopage>=0.4.0 in e:\\anaconda\\lib\\site-packages (from cliff->optuna) (0.5.1)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in e:\\anaconda\\lib\\site-packages (from cliff->optuna) (3.3.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in e:\\anaconda\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: pyperclip>=1.6 in e:\\anaconda\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
      "Requirement already satisfied: attrs>=16.3.0 in e:\\anaconda\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
      "Requirement already satisfied: pyreadline3 in e:\\anaconda\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in e:\\anaconda\\lib\\site-packages (from Mako->alembic->optuna) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\anaconda\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in e:\\anaconda\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\anaconda\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\anaconda\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2021.3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==1.10.2+cu113 torchvision==0.11.3+cu113 torchaudio===0.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "%pip install stable-baselines3[extra] optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "if not os.path.exists(OPT_DIR):\n",
    "    os.makedirs(OPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ppo(trial): \n",
    "    return {\n",
    "        'n_steps':trial.suggest_int('n_steps', 2048, 8192),\n",
    "        'gamma':trial.suggest_loguniform('gamma', 0.8, 0.9999),\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
    "        'clip_range':trial.suggest_uniform('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda':trial.suggest_uniform('gae_lambda', 0.8, 0.99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        model_params = optimize_ppo(trial) \n",
    "\n",
    "        # Create environment \n",
    "        env = BattleEnvironment()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        model = PPO('MlpPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        model.learn(total_timesteps=400000)\n",
    "\n",
    "        # Evaluate model \n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-26 23:48:40,083]\u001b[0m A new study created in memory with name: no-name-bddc5f02-555d-4fe2-b3e5-efade671b6fc\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2272`, after every 35 untruncated mini-batches, there will be a truncated mini-batch of size 32\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2272 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 00:09:38,402]\u001b[0m Trial 0 finished with value: -169.0 and parameters: {'n_steps': 2272, 'gamma': 0.9642980371346295, 'learning_rate': 1.8206091451363136e-05, 'clip_range': 0.3671029850123684, 'gae_lambda': 0.9642164753622319}. Best is trial 0 with value: -169.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6308`, after every 98 untruncated mini-batches, there will be a truncated mini-batch of size 36\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6308 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 00:30:24,947]\u001b[0m Trial 1 finished with value: -171.0 and parameters: {'n_steps': 6308, 'gamma': 0.8061914610173744, 'learning_rate': 2.781246766445887e-05, 'clip_range': 0.2814559385568447, 'gae_lambda': 0.9702712775527649}. Best is trial 0 with value: -169.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3402`, after every 53 untruncated mini-batches, there will be a truncated mini-batch of size 10\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3402 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 00:51:11,540]\u001b[0m Trial 2 finished with value: -171.0 and parameters: {'n_steps': 3402, 'gamma': 0.8746129796354307, 'learning_rate': 8.884082186678215e-05, 'clip_range': 0.15430156205968565, 'gae_lambda': 0.9420669864840594}. Best is trial 0 with value: -169.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2084`, after every 32 untruncated mini-batches, there will be a truncated mini-batch of size 36\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2084 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 01:11:53,028]\u001b[0m Trial 3 finished with value: -174.0 and parameters: {'n_steps': 2084, 'gamma': 0.8785298249482528, 'learning_rate': 4.2780938581594776e-05, 'clip_range': 0.3544391547652751, 'gae_lambda': 0.9648878466212167}. Best is trial 0 with value: -169.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6250`, after every 97 untruncated mini-batches, there will be a truncated mini-batch of size 42\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6250 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 01:32:44,540]\u001b[0m Trial 4 finished with value: 290.0 and parameters: {'n_steps': 6250, 'gamma': 0.9147121185824203, 'learning_rate': 7.749465757231603e-05, 'clip_range': 0.1080668156308741, 'gae_lambda': 0.8149783698111067}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4653`, after every 72 untruncated mini-batches, there will be a truncated mini-batch of size 45\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=4653 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 01:53:31,898]\u001b[0m Trial 5 finished with value: -169.0 and parameters: {'n_steps': 4653, 'gamma': 0.9424145155393516, 'learning_rate': 5.558120330219437e-05, 'clip_range': 0.24892689532369558, 'gae_lambda': 0.8568902089295556}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2459`, after every 38 untruncated mini-batches, there will be a truncated mini-batch of size 27\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2459 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 02:14:39,088]\u001b[0m Trial 6 finished with value: -184.0 and parameters: {'n_steps': 2459, 'gamma': 0.9206403541922218, 'learning_rate': 7.405101723829782e-05, 'clip_range': 0.1370677099296035, 'gae_lambda': 0.9568839491202327}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2452`, after every 38 untruncated mini-batches, there will be a truncated mini-batch of size 20\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2452 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 02:35:58,120]\u001b[0m Trial 7 finished with value: -171.0 and parameters: {'n_steps': 2452, 'gamma': 0.9443482970144597, 'learning_rate': 7.508987907557305e-05, 'clip_range': 0.25708454642252, 'gae_lambda': 0.8857785073318846}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2972`, after every 46 untruncated mini-batches, there will be a truncated mini-batch of size 28\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2972 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 02:57:09,301]\u001b[0m Trial 8 finished with value: -194.0 and parameters: {'n_steps': 2972, 'gamma': 0.96031829172164, 'learning_rate': 6.117957225113777e-05, 'clip_range': 0.3904840017957105, 'gae_lambda': 0.9347439933325091}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6762`, after every 105 untruncated mini-batches, there will be a truncated mini-batch of size 42\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6762 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 03:18:26,878]\u001b[0m Trial 9 finished with value: -190.0 and parameters: {'n_steps': 6762, 'gamma': 0.9230945832090571, 'learning_rate': 7.903828301073411e-05, 'clip_range': 0.33368187070215916, 'gae_lambda': 0.9184053651776382}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 7516`, after every 117 untruncated mini-batches, there will be a truncated mini-batch of size 28\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=7516 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 03:39:28,644]\u001b[0m Trial 10 finished with value: -173.0 and parameters: {'n_steps': 7516, 'gamma': 0.8368048169001028, 'learning_rate': 1.0623084302033936e-05, 'clip_range': 0.2055616476893246, 'gae_lambda': 0.8104183887276892}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5197`, after every 81 untruncated mini-batches, there will be a truncated mini-batch of size 13\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=5197 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 04:00:24,428]\u001b[0m Trial 11 finished with value: -157.0 and parameters: {'n_steps': 5197, 'gamma': 0.9944731517776914, 'learning_rate': 1.6897996741238067e-05, 'clip_range': 0.185289823675521, 'gae_lambda': 0.8080378170433798}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5345`, after every 83 untruncated mini-batches, there will be a truncated mini-batch of size 33\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=5345 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 04:21:23,322]\u001b[0m Trial 12 finished with value: -175.0 and parameters: {'n_steps': 5345, 'gamma': 0.995625424518484, 'learning_rate': 2.0626789268374194e-05, 'clip_range': 0.10579525976784179, 'gae_lambda': 0.8023841665869689}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4823`, after every 75 untruncated mini-batches, there will be a truncated mini-batch of size 23\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=4823 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 04:42:18,951]\u001b[0m Trial 13 finished with value: 68.0 and parameters: {'n_steps': 4823, 'gamma': 0.9918485429127251, 'learning_rate': 1.3829234905055254e-05, 'clip_range': 0.18262566053955837, 'gae_lambda': 0.8429944385391677}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4053`, after every 63 untruncated mini-batches, there will be a truncated mini-batch of size 21\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=4053 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 05:03:09,062]\u001b[0m Trial 14 finished with value: 52.0 and parameters: {'n_steps': 4053, 'gamma': 0.8997785060712408, 'learning_rate': 1.034709873557318e-05, 'clip_range': 0.10499237285725571, 'gae_lambda': 0.8444029602221116}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6129`, after every 95 untruncated mini-batches, there will be a truncated mini-batch of size 49\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6129 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 05:24:07,819]\u001b[0m Trial 15 finished with value: -193.0 and parameters: {'n_steps': 6129, 'gamma': 0.8504177366373382, 'learning_rate': 3.536941388452958e-05, 'clip_range': 0.19158465434329827, 'gae_lambda': 0.8440735492178444}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 7911`, after every 123 untruncated mini-batches, there will be a truncated mini-batch of size 39\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=7911 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 05:44:55,482]\u001b[0m Trial 16 finished with value: -177.0 and parameters: {'n_steps': 7911, 'gamma': 0.8972054588474736, 'learning_rate': 2.5798597995028955e-05, 'clip_range': 0.15133731348073684, 'gae_lambda': 0.8747804067452581}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4470`, after every 69 untruncated mini-batches, there will be a truncated mini-batch of size 54\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=4470 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 06:05:43,855]\u001b[0m Trial 17 finished with value: -166.0 and parameters: {'n_steps': 4470, 'gamma': 0.9748270569027977, 'learning_rate': 1.3330991668345488e-05, 'clip_range': 0.2078438664614632, 'gae_lambda': 0.8328231053743376}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5636`, after every 88 untruncated mini-batches, there will be a truncated mini-batch of size 4\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=5636 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 06:26:40,567]\u001b[0m Trial 18 finished with value: -181.0 and parameters: {'n_steps': 5636, 'gamma': 0.9222968331653009, 'learning_rate': 4.577895571579392e-05, 'clip_range': 0.13700008470855035, 'gae_lambda': 0.8293881851883829}. Best is trial 4 with value: 290.0.\u001b[0m\n",
      "e:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6831`, after every 106 untruncated mini-batches, there will be a truncated mini-batch of size 47\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6831 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-27 06:47:43,039]\u001b[0m Trial 19 finished with value: -159.0 and parameters: {'n_steps': 6831, 'gamma': 0.8475211435047714, 'learning_rate': 3.434418295236704e-05, 'clip_range': 0.23049046001447124, 'gae_lambda': 0.8675265119647686}. Best is trial 4 with value: 290.0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------- Load the Best Model ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params\n",
    "best_model = \"trial_4_best_model.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(os.path.join(OPT_DIR, best_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Callback ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Train the Model ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BattleEnvironment()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = study.best_params\n",
    "model_params['n_steps'] = (model_params['n_steps'] // 64) * 64 # set to a factor of 64\n",
    "model = PPO('MlpPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(os.path.join(OPT_DIR, best_model))\n",
    "model.learn(total_timesteps=4000000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Evaluation ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = BattleEnvironment()\n",
    "env.show = True\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "model = PPO.load(f\"{OPT_DIR}{best_model}\")\n",
    "\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=1)\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "for episode in range(episodes): # Evaluates the model n times\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    while not env.done:\n",
    "        action, _states = model.predict(state)\n",
    "        n_state, red_reward, done, info = env.step(action)\n",
    "        score += red_reward\n",
    "    print('Episode:{} Score:{}'.format(episode+1, score))\n",
    "env.env_method('show_wins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------- Render One Episode ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.show = True\n",
    "state = env.reset()\n",
    "score = 0\n",
    "while not train_env.done:\n",
    "    env.render()\n",
    "    action, _states = model.predict(state)\n",
    "    n_state, red_reward, done, info = env.step(action)\n",
    "    score += red_reward\n",
    "print('Score:{}'.format(score))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy_of_CAE_RL_battle_simulation_mar_25_2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
