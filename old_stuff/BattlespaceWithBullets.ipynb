{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rjkZl7XYZRZ"
   },
   "source": [
    "Working on implementing a Stable-Baselines3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pygame\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pygame.locals import *\n",
    "\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "DISP_WIDTH = 1000\n",
    "DISP_HEIGHT = 1000\n",
    "FPS = pygame.time.Clock()\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(p1, p0):\n",
    "    return math.degrees(math.atan2(p1[1]-p0[1],p1[0]-p0[0]))\n",
    "\n",
    "def dist(p1,p0):\n",
    "    return math.sqrt((p1[0]-p0[0])**2 + (p1[1]-p0[1])**2)\n",
    "\n",
    "def blitRotate(image, pos, originPos, angle):\n",
    "\n",
    "    # offset from pivot to center\n",
    "    image_rect = image.get_rect(topleft = (pos[0] - originPos[0], pos[1]-originPos[1]))\n",
    "    offset_center_to_pivot = pygame.math.Vector2(pos) - image_rect.center\n",
    "    \n",
    "    # roatated offset from pivot to center\n",
    "    rotated_offset = offset_center_to_pivot.rotate(-angle)\n",
    "\n",
    "    # rotated image center\n",
    "    rotated_image_center = (pos[0] - rotated_offset.x, pos[1] - rotated_offset.y)\n",
    "\n",
    "    # get a rotated image\n",
    "    rotated_image = pygame.transform.rotate(image, angle)\n",
    "    rotated_image_rect = rotated_image.get_rect(center = rotated_image_center)\n",
    "\n",
    "    return rotated_image, rotated_image_rect\n",
    "\n",
    "def calc_new_xy(old_xy, speed, time, angle):\n",
    "    new_x = old_xy[0] + (speed*time*math.cos(-math.radians(angle)))\n",
    "    new_y = old_xy[1] + (speed*time*math.sin(-math.radians(angle)))\n",
    "    return (new_x, new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 124,
     "status": "ok",
     "timestamp": 1654185833596,
     "user": {
      "displayName": "Shane Forry",
      "userId": "07873955147565922030"
     },
     "user_tz": 300
    },
    "id": "6UIhXw5Tn6Qf"
   },
   "outputs": [],
   "source": [
    "class Plane:\n",
    "    def __init__(self, team): \n",
    "        self.team = team\n",
    "        self.image = pygame.image.load(f\"Images/{team}_plane.png\")\n",
    "        self.w, self.h = self.image.get_size()\n",
    "        self.heading = 0\n",
    "        self.rect = self.image.get_rect()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        if self.team == 'red':\n",
    "            x = (DISP_WIDTH - self.w/2)/2 * random.random()\n",
    "            y = (DISP_HEIGHT - self.h/2) * random.random()\n",
    "            self.rect.center = (x, y)\n",
    "            self.heading = 90 * random.random() if random.random() < .5 else 90 * random.random() + 270\n",
    "            \n",
    "        else:\n",
    "            x = (DISP_WIDTH - self.w/2)/2 * random.random() + (DISP_WIDTH - self.w/2)/2\n",
    "            y = (DISP_HEIGHT - self.h/2) * random.random()\n",
    "            self.rect.center = (x, y)\n",
    "            self.heading = 180 * random.random() + 90\n",
    "        \n",
    "    def rotate(self, angle):\n",
    "        self.heading += angle\n",
    "\n",
    "    def set_heading(self, heading):\n",
    "        self.heading = heading\n",
    "\n",
    "    def forward(self, speed, time):\n",
    "        oldpos = self.rect.center\n",
    "        self.rect.center = calc_new_xy(oldpos, speed, time, self.heading)\n",
    "\n",
    "    def draw(self, surface):\n",
    "        image, rect = blitRotate(self.image, self.rect.center, (self.w/2, self.h/2), self.heading)\n",
    "        surface.blit(image, rect)\n",
    "\n",
    "    def update(self):\n",
    "        # Keep player on the screen\n",
    "        if self.rect.left < 0:\n",
    "            self.rect.left = 0\n",
    "        if self.rect.right > DISP_WIDTH:\n",
    "            self.rect.right = DISP_WIDTH\n",
    "        if self.rect.top <= 0:\n",
    "            self.rect.top = 0\n",
    "        if self.rect.bottom >= DISP_HEIGHT:\n",
    "            self.rect.bottom = DISP_HEIGHT\n",
    "\n",
    "    def get_pos(self):\n",
    "        return self.rect.center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    \n",
    "    def __init__(self, team):\n",
    "        self.team = team\n",
    "        self.image = pygame.image.load(f\"Images/{team}_base.png\")\n",
    "        self.w, self.h = self.image.get_size()\n",
    "        self.rect = self.image.get_rect()\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        if self.team == 'red':\n",
    "            x = (DISP_WIDTH - self.w/2)/2 * random.random()\n",
    "            y = (DISP_HEIGHT - self.h/2) * random.random()\n",
    "            self.rect.center = (x, y)\n",
    "        else:\n",
    "            x = (DISP_WIDTH - self.w/2)/2 * random.random() + (DISP_WIDTH - self.w/2)/2\n",
    "            y = (DISP_HEIGHT - self.h/2) * random.random()\n",
    "            self.rect.center = (x, y)\n",
    "\n",
    "    def draw(self, surface):\n",
    "        surface.blit(self.image, self.rect)\n",
    "            \n",
    "    def get_pos(self):\n",
    "        return self.rect.center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bullet(pygame.sprite.Sprite):\n",
    "    def __init__(self, x, y, angle, speed, fteam, oteam):\n",
    "        pygame.sprite.Sprite.__init__(self)\n",
    "        self.off_screen = False\n",
    "        self.image = pygame.Surface((8, 4), pygame.SRCALPHA)\n",
    "        self.fteam = fteam\n",
    "        self.color = RED if self.fteam == 'red' else BLUE\n",
    "        self.oteam = oteam\n",
    "        self.image.fill(self.color)\n",
    "        self.rect = self.image.get_rect(center=(x, y))\n",
    "        self.w, self.h = self.image.get_size()\n",
    "        self.heading = angle\n",
    "        self.pos = (x, y)\n",
    "        self.speed = speed\n",
    "\n",
    "    def update(self, screen_width, screen_height, time):\n",
    "        oldpos = self.rect.center\n",
    "        self.rect.center = calc_new_xy(oldpos, self.speed, time, self.heading)\n",
    "        if self.rect.centerx > screen_width or self.rect.centerx < 0 or self.rect.centery > screen_height or self.rect.centery < 0:\n",
    "            self.off_screen = True\n",
    "            return 'none'\n",
    "        for plane in self.oteam['planes']:\n",
    "            if self.rect.colliderect(plane.rect):\n",
    "                return 'plane'\n",
    "        if self.rect.colliderect(self.oteam['base'].rect):\n",
    "            return 'base'\n",
    "        return 'none'\n",
    "\n",
    "    def draw(self, surface):\n",
    "        image, rect = blitRotate(self.image, self.rect.center, (self.w/2, self.h/2), self.heading)\n",
    "        surface.blit(image, rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1662,
     "status": "ok",
     "timestamp": 1654185839436,
     "user": {
      "displayName": "Shane Forry",
      "userId": "07873955147565922030"
     },
     "user_tz": 300
    },
    "id": "tAEp-Jot_1qZ"
   },
   "outputs": [],
   "source": [
    "class BattlespaceEnv(gym.Env):\n",
    "    def __init__(self, show):\n",
    "        super(BattlespaceEnv, self).__init__()\n",
    "        self.show = show # Visualization\n",
    "\n",
    "        # For the Agent, actions are turn left, turn right, turn to enemy, turn to target, go forward, or shoot\n",
    "        # For the Random choice Agent, the actions are to enemy, to target, shoot enemy, or shoot target\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.random_action_space = [0, 1, 4, 5]\n",
    "        self.width = DISP_WIDTH\n",
    "        self.height = DISP_HEIGHT\n",
    "\n",
    "        high = np.array( # Observation space: fplane_pos_x, fplane_pos_y, fplane_angle, dist_obase, dist_oplane, rel_angle_obase, rel_angle_oplane\n",
    "            [\n",
    "                self.width,\n",
    "                self.height,\n",
    "                720,\n",
    "                math.sqrt(math.pow(self.width, 2) + math.pow(self.height, 2)),\n",
    "                math.sqrt(math.pow(self.width, 2) + math.pow(self.height, 2)),\n",
    "                720,\n",
    "                720,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        \n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.team = {}\n",
    "        self.team['red'] = {}\n",
    "        self.team['blue'] = {}\n",
    "        self.team['red']['base'] = Base('red')\n",
    "        self.team['blue']['base'] = Base('blue')\n",
    "        self.team['red']['planes'] = []\n",
    "        self.team['red']['planes'].append(Plane('red'))\n",
    "        self.team['blue']['planes'] = []\n",
    "        self.team['blue']['planes'].append(Plane('blue'))\n",
    "        self.team['red']['wins'] = 0\n",
    "        self.team['blue']['wins'] = 0\n",
    "\n",
    "        self.ties = 0\n",
    "\n",
    "\n",
    "        self.bullets = []\n",
    "\n",
    "        self.time_step = 0.1 # hours\n",
    "        self.speed = 500 # knots - nautical miles per hour\n",
    "        self.bullet_speed = 700\n",
    "        self.total_time = 0 # in hours\n",
    "        self.max_time = 10 # hours\n",
    "        self.shot_history = []\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # This code will all have to be changed when adding multiple planes\n",
    "        fplane = self.team['red']['planes'][0]\n",
    "        oplane = self.team['blue']['planes'][0]\n",
    "        obase = self.team['red']['base']\n",
    "\n",
    "        fplane_pos = fplane.get_pos()\n",
    "        fplane_angle = fplane.heading\n",
    "        oplane_pos = oplane.get_pos()\n",
    "        obase_pos = obase.get_pos()\n",
    "\n",
    "        dist_oplane = dist(oplane_pos, fplane_pos)\n",
    "        dist_obase = dist(obase_pos, fplane_pos)\n",
    "\n",
    "        angle_to_oplane = get_angle(oplane_pos, fplane_pos)\n",
    "        angle_to_obase = get_angle(obase_pos, fplane_pos)\n",
    "        rel_angle_oplane = (angle_to_oplane - fplane_angle) % 360\n",
    "        rel_angle_obase = (angle_to_obase - fplane_angle) % 360\n",
    "        \n",
    "        self.observation = (fplane_pos[0], fplane_pos[1], fplane_angle, dist_obase, dist_oplane, rel_angle_obase, rel_angle_oplane)\n",
    "        return np.array(self.observation, dtype=np.float32)\n",
    "\n",
    "        \n",
    "    def reset(self): # return observation\n",
    "        self.done = False\n",
    "        self.winner = 'none'\n",
    "\n",
    "        self.team['red']['base'].reset()\n",
    "        self.team['blue']['base'].reset()\n",
    "\n",
    "        for plane in self.team['red']['planes']:\n",
    "            plane.reset()\n",
    "        for plane in self.team['blue']['planes']:\n",
    "            plane.reset()\n",
    "\n",
    "        self.total_time = 0\n",
    "        self.bullets = []\n",
    "\n",
    "        if self.show:\n",
    "            pygame.init()\n",
    "            self.display = pygame.display.set_mode((DISP_WIDTH, DISP_HEIGHT))\n",
    "            pygame.display.set_caption(\"Battlespace Simulator\")\n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action): # return observation, reward, done, info\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        # Check if over time, if so, end game in tie\n",
    "        self.total_time += self.time_step\n",
    "        if self.total_time >= self.max_time:\n",
    "            self.done = True\n",
    "            self.ties += 1\n",
    "            if self.show:\n",
    "                self.render()\n",
    "                print(\"Draw\")\n",
    "            return self._get_observation(), 0, self.done, {}\n",
    "\n",
    "        # Red turn\n",
    "        self.friendly = 'red'\n",
    "        self.opponent = 'blue'\n",
    "        reward += self._process_action(action, self.team[self.friendly], self.team[self.opponent])\n",
    "        \n",
    "        # Blue turn\n",
    "        self.friendly = 'blue'\n",
    "        self.opponent = 'red'\n",
    "        self._process_action(self.random_action_space[random.randint(0, 3)], self.team[self.friendly], self.team[self.opponent])        \n",
    "        \n",
    "\n",
    "        # Check if bullets hit and move them\n",
    "        for bullet in self.bullets:\n",
    "            if not bullet.off_screen:\n",
    "                outcome = bullet.update(self.width, self.height, self.time_step)\n",
    "                if outcome == 'plane' or outcome == 'base': # If a bullet hit\n",
    "                    self.winner = bullet.fteam\n",
    "                    self.team[self.winner]['wins'] += 1\n",
    "                    self.done = True\n",
    "                    reward = reward + HIT_BASE_REWARD if outcome == 'base' else reward + HIT_PLANE_REWARD\n",
    "                    if self.show:\n",
    "                        self.render()\n",
    "                        print(f\"{self.winner} wins\")\n",
    "                    return self._get_observation(), reward, self.done, {}\n",
    "            else:\n",
    "                self.bullets.pop(self.bullets.index(bullet))\n",
    "    \n",
    "        # Continue game\n",
    "        if self.show:\n",
    "            self.render()\n",
    "        return self._get_observation(), reward, self.done, {}\n",
    "    \n",
    "    def _process_action(self, action, fteam, oteam): # friendly and opponent\n",
    "        reward = 0\n",
    "\n",
    "        fplane = fteam['planes'][0]\n",
    "        oplane = oteam['planes'][0]\n",
    "        obase = oteam['base']\n",
    "\n",
    "        fplane_pos = fplane.get_pos()\n",
    "        fplane_angle = fplane.heading\n",
    "        oplane_pos = oplane.get_pos()\n",
    "        obase_pos = obase.get_pos()\n",
    "\n",
    "        dist_oplane = dist(oplane_pos, fplane_pos)\n",
    "        dist_obase = dist(obase_pos, fplane_pos)\n",
    "\n",
    "        angle_to_oplane = get_angle(oplane_pos, fplane_pos)\n",
    "        angle_to_obase = get_angle(obase_pos, fplane_pos)\n",
    "        rel_angle_oplane = (angle_to_oplane - fplane_angle) % 360\n",
    "        rel_angle_obase = (angle_to_obase - fplane_angle) % 360\n",
    "\n",
    "        # --------------- FORWARDS ---------------\n",
    "        if action == 0: \n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "\n",
    "         # --------------- SHOOT ---------------\n",
    "        elif action == 1:\n",
    "            self.bullets.append(Bullet(fplane_pos[0], fplane_pos[1], fplane_angle, self.bullet_speed, self.friendly, oteam))\n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "        \n",
    "        # --------------- TURN RIGHT ---------------\n",
    "        elif action == 2:\n",
    "            fplane.rotate(-STEP_TURN)\n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "\n",
    "        # ---------------- TURN LEFT ----------------\n",
    "        elif action == 3:\n",
    "            fplane.rotate(STEP_TURN)\n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "\n",
    "        # ---------------- TURN TO OPLANE ----------------\n",
    "        elif action == 4:\n",
    "            if math.fabs(rel_angle_oplane) < STEP_TURN: # within step_turn of base\n",
    "                fplane.set_heading(angle_to_oplane)\n",
    "\n",
    "            elif math.fabs(rel_angle_oplane) > 360 - STEP_TURN: # within step_turn of base\n",
    "                fplane.set_heading(angle_to_oplane)\n",
    "\n",
    "            elif math.fabs(rel_angle_oplane) < 180: # turn right\n",
    "                fplane.rotate(-STEP_TURN)\n",
    "\n",
    "            else: # turn left\n",
    "                fplane.rotate(STEP_TURN)\n",
    "                \n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "\n",
    "        # ---------------- TURN TO OBASE ----------------\n",
    "        elif action == 5:\n",
    "            if math.fabs(rel_angle_obase) < STEP_TURN: # within step_turn of base\n",
    "                fplane.set_heading(angle_to_obase)\n",
    "\n",
    "            elif math.fabs(rel_angle_obase) > 360 - STEP_TURN: # within step_turn of base\n",
    "                fplane.set_heading(angle_to_obase)\n",
    "\n",
    "            elif math.fabs(rel_angle_obase) < 180: # turn right\n",
    "                fplane.rotate(-STEP_TURN)\n",
    "\n",
    "            else: # turn left\n",
    "                fplane.rotate(STEP_TURN)\n",
    "\n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "        \n",
    "        # ---------------- GIVE REWARDS IF CLOSER (DIST OR ANGLE) ----------------\n",
    "        new_fplane_pos = fplane.get_pos()\n",
    "        new_fplane_angle = fplane.heading\n",
    "        new_oplane_pos = oplane.get_pos()\n",
    "        new_obase_pos = obase.get_pos()\n",
    "\n",
    "        new_dist_oplane = dist(new_oplane_pos, new_fplane_pos)\n",
    "        new_dist_obase = dist(new_obase_pos, new_fplane_pos)\n",
    "\n",
    "        new_angle_to_oplane = get_angle(new_oplane_pos, new_fplane_pos)\n",
    "        new_angle_to_obase = get_angle(new_obase_pos, new_fplane_pos)\n",
    "        new_rel_angle_oplane = (new_angle_to_oplane - new_fplane_angle) % 360\n",
    "        new_rel_angle_obase = (new_angle_to_obase - new_fplane_angle) % 360\n",
    "\n",
    "        if new_dist_oplane < dist_oplane: # If got closer to enemy plane\n",
    "            reward += CLOSER_TO_PLANE_REWARD\n",
    "\n",
    "        if new_dist_obase < dist_obase: # If got closer to enemy base\n",
    "            reward += CLOSER_TO_BASE_REWARD\n",
    "\n",
    "        if math.fabs(new_rel_angle_oplane) < math.fabs(rel_angle_oplane): # If aiming closer to enemy plane\n",
    "            reward += TURN_TO_PLANE_REWARD\n",
    "\n",
    "        if math.fabs(new_rel_angle_obase) < math.fabs(rel_angle_obase): # If aiming closer to enemy base\n",
    "            reward += TURN_TO_BASE_REWARD\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def draw_shot(self, hit, friendly_pos, target_pos, team):\n",
    "        color = (0, 0, 0)\n",
    "        color = (255, 0, 0) if team == 'red' else (0, 0, 255)\n",
    "        if not hit: target_pos = (target_pos[0] + (random.random() * 2 - 1) * 100, target_pos[1] + (random.random() * 2 - 1) * 100)\n",
    "        self.shot_history.append((hit, friendly_pos, target_pos, color))\n",
    "    \n",
    "    def winner_screen(self):\n",
    "        if self.show:\n",
    "            font = pygame.font.Font('freesansbold.ttf', 32)\n",
    "            if self.winner != 'none':\n",
    "                text = font.render(f\"THE WINNER IS {self.winner.upper()}\", True, (0, 0, 0))\n",
    "                textRect = text.get_rect()\n",
    "                textRect.center = (DISP_WIDTH//2, DISP_HEIGHT//2)\n",
    "            else:\n",
    "                text = font.render(f\"THE GAME IS A TIE\", True, (0, 0, 0))\n",
    "                textRect = text.get_rect()\n",
    "                textRect.center = (DISP_WIDTH//2, DISP_HEIGHT//2)\n",
    "            self.display.blit(text, textRect)\n",
    "\n",
    "    def show_wins(self):\n",
    "        print(\"Wins by red:\", self.team['red']['wins'])\n",
    "        print(\"Wins by blue:\", self.team['blue']['wins'])\n",
    "        print(\"Tied games:\", self.ties)\n",
    "\n",
    "    def render(self):\n",
    "        if self.show: # Just to ensure it won't render if self.show == False\n",
    "            for event in pygame.event.get():\n",
    "                # Check for KEYDOWN event\n",
    "                if event.type == KEYDOWN:\n",
    "                    # If the Esc key is pressed, then exit the main loop\n",
    "                    if event.key == K_ESCAPE:\n",
    "                        pygame.quit()\n",
    "                        sys.exit()\n",
    "                        return\n",
    "                # Check for QUIT event. If QUIT, then set running to false.\n",
    "                elif event.type == QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "                    return\n",
    "                    \n",
    "            # Fill background\n",
    "            self.display.fill(WHITE)\n",
    "\n",
    "            # Draw bullets\n",
    "            for bullet in self.bullets:\n",
    "                bullet.draw(self.display)\n",
    "                    \n",
    "            # Draw bases\n",
    "            self.team['red']['base'].draw(self.display)\n",
    "            self.team['blue']['base'].draw(self.display)\n",
    "\n",
    "            # Draw planes\n",
    "            for plane in self.team['red']['planes']:\n",
    "                plane.update()\n",
    "                plane.draw(self.display)\n",
    "            for plane in self.team['blue']['planes']:\n",
    "                plane.update()\n",
    "                plane.draw(self.display)\n",
    "\n",
    "            # Winner Screen\n",
    "            if self.done:\n",
    "                font = pygame.font.Font('freesansbold.ttf', 32)\n",
    "                if self.winner != 'none':\n",
    "                    text = font.render(f\"THE WINNER IS {self.winner.upper()}\", True, (0, 0, 0))\n",
    "                    textRect = text.get_rect()\n",
    "                    textRect.center = (DISP_WIDTH//2, DISP_HEIGHT//2)\n",
    "                else:\n",
    "                    text = font.render(f\"THE GAME IS A TIE\", True, (0, 0, 0))\n",
    "                    textRect = text.get_rect()\n",
    "                    textRect.center = (DISP_WIDTH//2, DISP_HEIGHT//2)\n",
    "                self.display.blit(text, textRect)\n",
    "                pygame.display.update()\n",
    "                pygame.time.wait(3000)\n",
    "                pygame.quit()\n",
    "                return\n",
    "            \n",
    "            pygame.display.update()\n",
    "            FPS.tick(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- THESE REWARD VALUES SHOULD ENSURE THAT THE AGENT IS WILLING TO SHOOT -------------\n",
    "\n",
    "In other words, there's a high probability of hitting and little/no punishment for missing a shot\n",
    "Since this is likely the first round of training, it's okay if the agent shoots too often"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note- You can look at the comparison graphs using the following in an anaconda terminal in this folder:\n",
    "tensorboard --logdir=logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_TURN = 30 # degrees (amt to turn per step)\n",
    "HIT_BASE_REWARD = 10000\n",
    "HIT_PLANE_REWARD = 10000\n",
    "MISS_PUNISHMENT = 0\n",
    "CLOSER_TO_BASE_REWARD = 3\n",
    "CLOSER_TO_PLANE_REWARD = 3\n",
    "TURN_TO_BASE_REWARD = 3\n",
    "TURN_TO_PLANE_REWARD = 3\n",
    "\n",
    "train_env = BattleEnvironment(False)\n",
    "logdir = \"logs\"\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "TIMESTEPS = 50000 # Saves every n steps\n",
    "ITERATIONS = 10 # Times saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to logs\\PPO_0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.8     |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    fps             | 387      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90.1        |\n",
      "|    ep_rew_mean          | 200         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 421         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011407625 |\n",
      "|    clip_fraction        | 0.0948      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | -0.00289    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 67.3        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00642    |\n",
      "|    value_loss           | 6.48e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 87.8        |\n",
      "|    ep_rew_mean          | 227         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 434         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006777853 |\n",
      "|    clip_fraction        | 0.0104      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.000603    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 116         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0049     |\n",
      "|    value_loss           | 9.59e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 89           |\n",
      "|    ep_rew_mean          | 229          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073571913 |\n",
      "|    clip_fraction        | 0.0691       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.76        |\n",
      "|    explained_variance   | 0.00402      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.7e+03      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00822     |\n",
      "|    value_loss           | 1.64e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 241         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 427         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011253639 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.00614     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 140         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 1.08e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 89          |\n",
      "|    ep_rew_mean          | 264         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 433         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009504289 |\n",
      "|    clip_fraction        | 0.0204      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.00681     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.18e+03    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    value_loss           | 1.28e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 89           |\n",
      "|    ep_rew_mean          | 277          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 425          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068449927 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 0.0061       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.99e+03     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00469     |\n",
      "|    value_loss           | 1.67e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90.2        |\n",
      "|    ep_rew_mean          | 287         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 428         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009924667 |\n",
      "|    clip_fraction        | 0.0405      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.00436     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.01e+03    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    value_loss           | 9.47e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 84.4        |\n",
      "|    ep_rew_mean          | 340         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 434         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005777453 |\n",
      "|    clip_fraction        | 0.0468      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.00608     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 281         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00988    |\n",
      "|    value_loss           | 1.01e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 86.3         |\n",
      "|    ep_rew_mean          | 331          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 436          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058366386 |\n",
      "|    clip_fraction        | 0.0148       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.51        |\n",
      "|    explained_variance   | 0.00405      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.04e+04     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00527     |\n",
      "|    value_loss           | 2.2e+04      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 87          |\n",
      "|    ep_rew_mean          | 338         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 427         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013006922 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.0096      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.49e+03    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00725    |\n",
      "|    value_loss           | 8.29e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 88.2        |\n",
      "|    ep_rew_mean          | 340         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 430         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007336387 |\n",
      "|    clip_fraction        | 0.0738      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.0178      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 169         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00557    |\n",
      "|    value_loss           | 8.27e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 94.4         |\n",
      "|    ep_rew_mean          | 281          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 433          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026192553 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.0155       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+04     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00486     |\n",
      "|    value_loss           | 1.51e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 93           |\n",
      "|    ep_rew_mean          | 289          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 428          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046564764 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0259       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.23e+04     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00571     |\n",
      "|    value_loss           | 3.88e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 92.9         |\n",
      "|    ep_rew_mean          | 279          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 431          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022280957 |\n",
      "|    clip_fraction        | 0.0242       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.02         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.37e+03     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00613     |\n",
      "|    value_loss           | 8.81e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 94.4         |\n",
      "|    ep_rew_mean          | 262          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 434          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051273154 |\n",
      "|    clip_fraction        | 0.0432       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.0226       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.3e+03      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00383     |\n",
      "|    value_loss           | 6.36e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 92.3         |\n",
      "|    ep_rew_mean          | 288          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 430          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060978374 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.0222       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 446          |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00553     |\n",
      "|    value_loss           | 4.33e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 92.3         |\n",
      "|    ep_rew_mean          | 297          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 428          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 86           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049780617 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.0205       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.07e+04     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00637     |\n",
      "|    value_loss           | 2.1e+04      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 88.4         |\n",
      "|    ep_rew_mean          | 354          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 413          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 94           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031737813 |\n",
      "|    clip_fraction        | 0.00498      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0.0507       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.63e+03     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00349     |\n",
      "|    value_loss           | 1.31e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 85.9         |\n",
      "|    ep_rew_mean          | 379          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 395          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 103          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043673925 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0.0335       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.04e+03     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00533     |\n",
      "|    value_loss           | 2.11e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 86.7         |\n",
      "|    ep_rew_mean          | 374          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 373          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 115          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027300841 |\n",
      "|    clip_fraction        | 0.00937      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.0453       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 268          |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00404     |\n",
      "|    value_loss           | 1.16e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 86.7         |\n",
      "|    ep_rew_mean          | 375          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 351          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 128          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029137977 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.0595       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.51e+04     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    value_loss           | 1.49e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 86.3        |\n",
      "|    ep_rew_mean          | 376         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 335         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005327708 |\n",
      "|    clip_fraction        | 0.0241      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | -0.00292    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.26e+03    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00378    |\n",
      "|    value_loss           | 1.54e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 88.2        |\n",
      "|    ep_rew_mean          | 390         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 327         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005857487 |\n",
      "|    clip_fraction        | 0.0215      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.0392      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.27e+03    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    value_loss           | 1.03e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 84.9         |\n",
      "|    ep_rew_mean          | 435          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 324          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 157          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046790587 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.0034       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81e+04     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    value_loss           | 2.86e+04     |\n",
      "------------------------------------------\n",
      "Wins by red: 64\n",
      "Wins by blue: 39\n",
      "Tied games: 475\n"
     ]
    }
   ],
   "source": [
    "model_name = \"PPO\"\n",
    "models_dir = f\"models/{model_name}\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", train_env, verbose=1, tensorboard_log=logdir)\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"{model_name}\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*(i+1)}\")\n",
    "\n",
    "train_env.show_wins()\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and run for x episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models\\\\PPO\\\\200000.zip.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Willi\\OneDrive\\Research\\BattlespaceWithBullets.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Willi/OneDrive/Research/BattlespaceWithBullets.ipynb#ch0000013?line=1'>2</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Willi/OneDrive/Research/BattlespaceWithBullets.ipynb#ch0000013?line=2'>3</a>\u001b[0m models_dir \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Willi/OneDrive/Research/BattlespaceWithBullets.ipynb#ch0000013?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m PPO\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodels_dir\u001b[39m}\u001b[39;49;00m\u001b[39m/200000.zip\u001b[39;49m\u001b[39m\"\u001b[39;49m, env \u001b[39m=\u001b[39;49m eval_env) \u001b[39m# Loads the saved model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Willi/OneDrive/Research/BattlespaceWithBullets.ipynb#ch0000013?line=6'>7</a>\u001b[0m episodes \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Willi/OneDrive/Research/BattlespaceWithBullets.ipynb#ch0000013?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(episodes): \u001b[39m# Evaluates the model n times\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Willi\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:687\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    684\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    685\u001b[0m     get_system_info()\n\u001b[1;32m--> 687\u001b[0m data, params, pytorch_variables \u001b[39m=\u001b[39m load_from_zip_file(\n\u001b[0;32m    688\u001b[0m     path, device\u001b[39m=\u001b[39;49mdevice, custom_objects\u001b[39m=\u001b[39;49mcustom_objects, print_system_info\u001b[39m=\u001b[39;49mprint_system_info\n\u001b[0;32m    689\u001b[0m )\n\u001b[0;32m    691\u001b[0m \u001b[39m# Remove stored device information and replace with ours\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mpolicy_kwargs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m data:\n",
      "File \u001b[1;32mc:\\Users\\Willi\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:388\u001b[0m, in \u001b[0;36mload_from_zip_file\u001b[1;34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_zip_file\u001b[39m(\n\u001b[0;32m    362\u001b[0m     load_path: Union[\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPath, io\u001b[39m.\u001b[39mBufferedIOBase],\n\u001b[0;32m    363\u001b[0m     load_data: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    367\u001b[0m     print_system_info: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    368\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (Tuple[Optional[Dict[\u001b[39mstr\u001b[39m, Any]], Optional[TensorDict], Optional[TensorDict]]):\n\u001b[0;32m    369\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[39m    Load model data from a .zip archive\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39m        and dict of pytorch variables\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 388\u001b[0m     load_path \u001b[39m=\u001b[39m open_path(load_path, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, verbose\u001b[39m=\u001b[39;49mverbose, suffix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    390\u001b[0m     \u001b[39m# set device to cpu if cuda is not available\u001b[39;00m\n\u001b[0;32m    391\u001b[0m     device \u001b[39m=\u001b[39m get_device(device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Willi\\anaconda3\\lib\\functools.py:888\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    886\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 888\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Willi\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:232\u001b[0m, in \u001b[0;36mopen_path_str\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39m@open_path\u001b[39m\u001b[39m.\u001b[39mregister(\u001b[39mstr\u001b[39m)\n\u001b[0;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen_path_str\u001b[39m(path: \u001b[39mstr\u001b[39m, mode: \u001b[39mstr\u001b[39m, verbose: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, suffix: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m io\u001b[39m.\u001b[39mBufferedIOBase:\n\u001b[0;32m    219\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m    Open a path given by a string. If writing to the path, the function ensures\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39m    that the path exists.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39m    :return:\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m     \u001b[39mreturn\u001b[39;00m open_path(pathlib\u001b[39m.\u001b[39;49mPath(path), mode, verbose, suffix)\n",
      "File \u001b[1;32mc:\\Users\\Willi\\anaconda3\\lib\\functools.py:888\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    886\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 888\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Willi\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:284\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    277\u001b[0m         path\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mmkdir(exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    279\u001b[0m \u001b[39m# if opening was successful uses the identity function\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[39m# if opening failed with IsADirectory|FileNotFound, calls open_path_pathlib\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[39m#   with corrections\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[39m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m open_path(path, mode, verbose, suffix)\n",
      "File \u001b[1;32mc:\\Users\\Willi\\anaconda3\\lib\\functools.py:888\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    886\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 888\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Willi\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:264\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    262\u001b[0m             path, suffix \u001b[39m=\u001b[39m newpath, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    263\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m             \u001b[39mraise\u001b[39;00m error\n\u001b[0;32m    265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Willi\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:256\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m         path \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39;49mopen(\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    257\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m    258\u001b[0m         \u001b[39mif\u001b[39;00m suffix \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m suffix \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Willi\\anaconda3\\lib\\pathlib.py:1252\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, buffering\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1247\u001b[0m          errors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, newline\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1248\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m \u001b[39m    Open the file pointed by this path and return a file object, as\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[39m    the built-in open() function does.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1252\u001b[0m     \u001b[39mreturn\u001b[39;00m io\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m, mode, buffering, encoding, errors, newline,\n\u001b[0;32m   1253\u001b[0m                    opener\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_opener)\n",
      "File \u001b[1;32mc:\\Users\\Willi\\anaconda3\\lib\\pathlib.py:1120\u001b[0m, in \u001b[0;36mPath._opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_opener\u001b[39m(\u001b[39mself\u001b[39m, name, flags, mode\u001b[39m=\u001b[39m\u001b[39m0o666\u001b[39m):\n\u001b[0;32m   1119\u001b[0m     \u001b[39m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m, flags, mode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models\\\\PPO\\\\200000.zip.zip'"
     ]
    }
   ],
   "source": [
    "eval_env = BattleEnvironment(True)\n",
    "model_name = \"PPO\"\n",
    "models_dir = f\"models/{model_name}\"\n",
    "\n",
    "model = PPO.load(f\"{models_dir}/200000.zip\", env = eval_env) # Loads the saved model\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(episodes): # Evaluates the model n times\n",
    "    state = eval_env.reset()\n",
    "    score = 0\n",
    "    while not eval_env.done:\n",
    "        # action, _states = model.predict(state)\n",
    "        action = eval_env.action_space.sample()\n",
    "        n_state, red_reward, done, info = eval_env.step(action)\n",
    "        score += red_reward\n",
    "    print('Episode:{} Score:{}'.format(episode+1, score))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy_of_CAE_RL_battle_simulation_mar_25_2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b5efcd3efc71ef29576cdfc4a5c5091a22a4d39f277c681ebc64abd29d3aec9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
