{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\willi\\anaconda3\\lib\\site-packages (2.1.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\willi\\anaconda3\\lib\\site-packages (1.21.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: gym in c:\\users\\willi\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\willi\\anaconda3\\lib\\site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\willi\\anaconda3\\lib\\site-packages (from gym) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pygame\n",
    "%pip install numpy\n",
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "from gym import spaces\n",
    "import os\n",
    "import sys\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "DISP_WIDTH = 1000\n",
    "DISP_HEIGHT = 1000\n",
    "FPS = pygame.time.Clock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Default Config ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG_DICT = {\n",
    "    'time_step': 0.1, # hours per time step\n",
    "    'plane_speed': 500, # mph\n",
    "    'bullet_speed': 700, # mph\n",
    "    'max_time': 10, # hours the epoch can last\n",
    "    'show_viz': False, # show the pygame animation\n",
    "    'step_turn': 30, # degrees to turn per step\n",
    "    'hit_base_reward': 1000, # reward for shooting enemy base\n",
    "    'hit_plane_reward': 1000, # reward for shooting enemy plane\n",
    "    'miss_punishment': -5, # punishment for missing a shot\n",
    "    'too_long_punishment': -1, # punishment for taking too long to end the game\n",
    "    'closer_to_base_reward': 0, # reward for getting closer to enemy base\n",
    "    'closer_to_plane_reward': 0, # reward for getting closer to enemy plane\n",
    "    'turn_to_base_reward': 0, # reward for turning towards the enemy base\n",
    "    'turn_to_plane_reward': 0 # reward for turning towards the enemy plane\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Helper Functions ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(p1, p0):\n",
    "    return math.degrees(math.atan2(p1[1]-p0[1],p1[0]-p0[0]))\n",
    "\n",
    "def dist(p1,p0):\n",
    "    return math.sqrt((p1[0]-p0[0])**2 + (p1[1]-p0[1])**2)\n",
    "\n",
    "def blitRotate(image, pos, originPos, angle):\n",
    "\n",
    "    # offset from pivot to center\n",
    "    image_rect = image.get_rect(topleft = (pos[0] - originPos[0], pos[1]-originPos[1]))\n",
    "    offset_center_to_pivot = pygame.math.Vector2(pos) - image_rect.center\n",
    "    \n",
    "    # roatated offset from pivot to center\n",
    "    rotated_offset = offset_center_to_pivot.rotate(-angle)\n",
    "\n",
    "    # rotated image center\n",
    "    rotated_image_center = (pos[0] - rotated_offset.x, pos[1] - rotated_offset.y)\n",
    "\n",
    "    # get a rotated image\n",
    "    rotated_image = pygame.transform.rotate(image, angle)\n",
    "    rotated_image_rect = rotated_image.get_rect(center = rotated_image_center)\n",
    "\n",
    "    return rotated_image, rotated_image_rect\n",
    "\n",
    "def calc_new_xy(old_xy, speed, time, angle):\n",
    "    new_x = old_xy[0] + (speed*time*math.cos(-math.radians(angle)))\n",
    "    new_y = old_xy[1] + (speed*time*math.sin(-math.radians(angle)))\n",
    "    return (new_x, new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Plane Class ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 124,
     "status": "ok",
     "timestamp": 1654185833596,
     "user": {
      "displayName": "Shane Forry",
      "userId": "07873955147565922030"
     },
     "user_tz": 300
    },
    "id": "6UIhXw5Tn6Qf"
   },
   "outputs": [],
   "source": [
    "class Plane:\n",
    "    def __init__(self, team): \n",
    "        self.team = team\n",
    "        self.image = pygame.image.load(f\"Images/{team}_plane.png\")\n",
    "        self.w, self.h = self.image.get_size()\n",
    "        self.heading = 0\n",
    "        self.rect = self.image.get_rect()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        if self.team == 'red':\n",
    "            x = (DISP_WIDTH - self.w/2)/2 * random.random()\n",
    "            y = (DISP_HEIGHT - self.h/2) * random.random()\n",
    "            self.rect.center = (x, y)\n",
    "            self.heading = 90 * random.random() if random.random() < .5 else 90 * random.random() + 270\n",
    "            \n",
    "        else:\n",
    "            x = (DISP_WIDTH - self.w/2)/2 * random.random() + (DISP_WIDTH - self.w/2)/2\n",
    "            y = (DISP_HEIGHT - self.h/2) * random.random()\n",
    "            self.rect.center = (x, y)\n",
    "            self.heading = 180 * random.random() + 90\n",
    "        \n",
    "    def rotate(self, angle):\n",
    "        self.heading += angle\n",
    "\n",
    "    def set_heading(self, heading):\n",
    "        self.heading = heading\n",
    "\n",
    "    def forward(self, speed, time):\n",
    "        oldpos = self.rect.center\n",
    "        self.rect.center = calc_new_xy(oldpos, speed, time, self.heading)\n",
    "\n",
    "    def draw(self, surface):\n",
    "        image, rect = blitRotate(self.image, self.rect.center, (self.w/2, self.h/2), self.heading)\n",
    "        surface.blit(image, rect)\n",
    "\n",
    "    def update(self):\n",
    "        # Keep player on the screen\n",
    "        if self.rect.left < 0:\n",
    "            self.rect.left = 0\n",
    "        if self.rect.right > DISP_WIDTH:\n",
    "            self.rect.right = DISP_WIDTH\n",
    "        if self.rect.top <= 0:\n",
    "            self.rect.top = 0\n",
    "        if self.rect.bottom >= DISP_HEIGHT:\n",
    "            self.rect.bottom = DISP_HEIGHT\n",
    "\n",
    "    def get_pos(self):\n",
    "        return self.rect.center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------- Base Class ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    \n",
    "    def __init__(self, team):\n",
    "        self.team = team\n",
    "        self.image = pygame.image.load(f\"Images/{team}_base.png\")\n",
    "        self.w, self.h = self.image.get_size()\n",
    "        self.rect = self.image.get_rect()\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        if self.team == 'red':\n",
    "            x = (DISP_WIDTH - self.w/2)/2 * random.random()\n",
    "            y = (DISP_HEIGHT - self.h/2) * random.random()\n",
    "            self.rect.center = (x, y)\n",
    "        else:\n",
    "            x = (DISP_WIDTH - self.w/2)/2 * random.random() + (DISP_WIDTH - self.w/2)/2\n",
    "            y = (DISP_HEIGHT - self.h/2) * random.random()\n",
    "            self.rect.center = (x, y)\n",
    "\n",
    "    def draw(self, surface):\n",
    "        surface.blit(self.image, self.rect)\n",
    "            \n",
    "    def get_pos(self):\n",
    "        return self.rect.center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Bullet Class ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bullet(pygame.sprite.Sprite):\n",
    "    def __init__(self, x, y, angle, speed, fteam, oteam):\n",
    "        pygame.sprite.Sprite.__init__(self)\n",
    "        self.off_screen = False\n",
    "        self.image = pygame.Surface((8, 4), pygame.SRCALPHA)\n",
    "        self.fteam = fteam\n",
    "        self.color = RED if self.fteam == 'red' else BLUE\n",
    "        self.oteam = oteam\n",
    "        self.image.fill(self.color)\n",
    "        self.rect = self.image.get_rect(center=(x, y))\n",
    "        self.w, self.h = self.image.get_size()\n",
    "        self.heading = angle + (random.random() * 10 - 5)\n",
    "        self.pos = (x, y)\n",
    "        self.speed = speed\n",
    "\n",
    "    def update(self, screen_width, screen_height, time):\n",
    "        oldpos = self.rect.center\n",
    "        self.rect.center = calc_new_xy(oldpos, self.speed, time, self.heading)\n",
    "        if self.rect.centerx > screen_width or self.rect.centerx < 0 or self.rect.centery > screen_height or self.rect.centery < 0:\n",
    "            return 'miss'\n",
    "        for plane in self.oteam['planes']:\n",
    "            if self.rect.colliderect(plane.rect):\n",
    "                return 'plane'\n",
    "        if self.rect.colliderect(self.oteam['base'].rect):\n",
    "            return 'base'\n",
    "        return 'none'\n",
    "\n",
    "    def draw(self, surface):\n",
    "        image, rect = blitRotate(self.image, self.rect.center, (self.w/2, self.h/2), self.heading)\n",
    "        surface.blit(image, rect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------- Battle Environment ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1662,
     "status": "ok",
     "timestamp": 1654185839436,
     "user": {
      "displayName": "Shane Forry",
      "userId": "07873955147565922030"
     },
     "user_tz": 300
    },
    "id": "tAEp-Jot_1qZ"
   },
   "outputs": [],
   "source": [
    "class BattleEnvironment(gym.Env):\n",
    "    def __init__(self, config: dict=DEFAULT_CONFIG_DICT):\n",
    "        super(BattleEnvironment, self).__init__()\n",
    "        self.width = DISP_WIDTH\n",
    "        self.height = DISP_HEIGHT\n",
    "        self.max_time = config['max_time']\n",
    "        high = np.array( # Observation space: fplane_pos_x, fplane_pos_y, fplane_angle, dist_obase, dist_oplane, rel_angle_obase, rel_angle_oplane\n",
    "            [\n",
    "                self.width,\n",
    "                self.height,\n",
    "                720,\n",
    "                math.sqrt(math.pow(self.width, 2) + math.pow(self.height, 2)),\n",
    "                math.sqrt(math.pow(self.width, 2) + math.pow(self.height, 2)),\n",
    "                720,\n",
    "                720,\n",
    "                self.max_time,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        # For the Agent, actions are turn left, turn right, turn to enemy, turn to target, go forward, or shoot\n",
    "        # For the Random choice Agent, the actions are to enemy, to target, shoot enemy, or shoot target\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.random_action_space = [0, 1, 4, 5]\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "        self.team = {}\n",
    "        self.team['red'] = {}\n",
    "        self.team['blue'] = {}\n",
    "        self.team['red']['base'] = Base('red')\n",
    "        self.team['blue']['base'] = Base('blue')\n",
    "        self.team['red']['planes'] = []\n",
    "        self.team['red']['planes'].append(Plane('red'))\n",
    "        self.team['blue']['planes'] = []\n",
    "        self.team['blue']['planes'].append(Plane('blue'))\n",
    "        self.team['red']['wins'] = 0\n",
    "        self.team['blue']['wins'] = 0\n",
    "        self.ties = 0\n",
    "        self.bullets = []\n",
    "        self.speed = config['plane_speed']\n",
    "        self.bullet_speed = config['bullet_speed']\n",
    "        self.total_time = 0 # in hours\n",
    "        self.time_step = config['time_step']\n",
    "        self.show = config['show_viz']\n",
    "        self.step_turn = config['step_turn']\n",
    "        self.hit_base_reward = config['hit_base_reward']\n",
    "        self.hit_plane_reward = config['hit_plane_reward']\n",
    "        self.miss_punishment = config['miss_punishment']\n",
    "        self.too_long_punishment = config['too_long_punishment']\n",
    "        self.closer_to_base_reward = config['closer_to_base_reward']\n",
    "        self.closer_to_plane_reward = config['closer_to_plane_reward']\n",
    "        self.turn_to_base_reward = config['turn_to_base_reward']\n",
    "        self.turn_to_plane_reward = config['turn_to_plane_reward']\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # This code will all have to be changed when adding multiple planes\n",
    "        fplane = self.team['red']['planes'][0]\n",
    "        oplane = self.team['blue']['planes'][0]\n",
    "        obase = self.team['red']['base']\n",
    "\n",
    "        fplane_pos = fplane.get_pos()\n",
    "        fplane_angle = fplane.heading\n",
    "        oplane_pos = oplane.get_pos()\n",
    "        obase_pos = obase.get_pos()\n",
    "\n",
    "        dist_oplane = dist(oplane_pos, fplane_pos)\n",
    "        dist_obase = dist(obase_pos, fplane_pos)\n",
    "\n",
    "        angle_to_oplane = get_angle(oplane_pos, fplane_pos)\n",
    "        angle_to_obase = get_angle(obase_pos, fplane_pos)\n",
    "        rel_angle_oplane = (angle_to_oplane - fplane_angle) % 360\n",
    "        rel_angle_obase = (angle_to_obase - fplane_angle) % 360\n",
    "        \n",
    "        self.observation = (fplane_pos[0], fplane_pos[1], fplane_angle, dist_obase, dist_oplane, rel_angle_obase, rel_angle_oplane, self.total_time)\n",
    "        return np.array(self.observation, dtype=np.float32)\n",
    "\n",
    "        \n",
    "    def reset(self): # return observation\n",
    "        self.done = False\n",
    "        self.winner = 'none'\n",
    "\n",
    "        self.team['red']['base'].reset()\n",
    "        self.team['blue']['base'].reset()\n",
    "\n",
    "        for plane in self.team['red']['planes']:\n",
    "            plane.reset()\n",
    "        for plane in self.team['blue']['planes']:\n",
    "            plane.reset()\n",
    "\n",
    "        self.total_time = 0\n",
    "        self.bullets = []\n",
    "\n",
    "        if self.show:\n",
    "            pygame.init()\n",
    "            self.display = pygame.display.set_mode((DISP_WIDTH, DISP_HEIGHT))\n",
    "            pygame.display.set_caption(\"Battlespace Simulator\")\n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action): # return observation, reward, done, info\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        # Check if over time, if so, end game in tie\n",
    "        self.total_time += self.time_step\n",
    "        if self.total_time >= self.max_time:\n",
    "            self.done = True\n",
    "            self.ties += 1\n",
    "            if self.show:\n",
    "                self.render()\n",
    "                print(\"Draw\")\n",
    "            return self._get_observation(), 0, self.done, {}\n",
    "\n",
    "        # Red turn\n",
    "        self.friendly = 'red'\n",
    "        self.opponent = 'blue'\n",
    "        reward += self._process_action(action, self.team[self.friendly], self.team[self.opponent])\n",
    "        \n",
    "        # Blue turn\n",
    "        self.friendly = 'blue'\n",
    "        self.opponent = 'red'\n",
    "        self._process_action(self.random_action_space[random.randint(0, 3)], self.team[self.friendly], self.team[self.opponent])        \n",
    "\n",
    "        # Check if bullets hit and move them\n",
    "        for bullet in self.bullets:\n",
    "            outcome = bullet.update(self.width, self.height, self.time_step)\n",
    "            if outcome == 'miss':\n",
    "                reward += self.miss_punishment\n",
    "                self.bullets.pop(self.bullets.index(bullet))\n",
    "            elif outcome == 'plane' or outcome == 'base': # If a bullet hit\n",
    "                self.winner = bullet.fteam\n",
    "                self.team[self.winner]['wins'] += 1\n",
    "                self.done = True\n",
    "                reward = reward + self.hit_base_reward if outcome == 'base' else reward + self.hit_plane_reward\n",
    "                if self.show:\n",
    "                    self.render()\n",
    "                    print(f\"{self.winner} wins\")\n",
    "                return self._get_observation(), reward, self.done, {}\n",
    "            \n",
    "        # Check if past half of max time and give punishment\n",
    "        if (self.total_time > self.max_time//2):\n",
    "            reward += self.too_long_punishment\n",
    "\n",
    "        # Continue game\n",
    "        if self.show:\n",
    "            self.render()\n",
    "        return self._get_observation(), reward, self.done, {}\n",
    "    \n",
    "    def _process_action(self, action, fteam, oteam): # friendly and opponent\n",
    "        reward = 0\n",
    "\n",
    "        fplane = fteam['planes'][0]\n",
    "        oplane = oteam['planes'][0]\n",
    "        obase = oteam['base']\n",
    "\n",
    "        fplane_pos = fplane.get_pos()\n",
    "        fplane_angle = fplane.heading\n",
    "        oplane_pos = oplane.get_pos()\n",
    "        obase_pos = obase.get_pos()\n",
    "\n",
    "        dist_oplane = dist(oplane_pos, fplane_pos)\n",
    "        dist_obase = dist(obase_pos, fplane_pos)\n",
    "\n",
    "        angle_to_oplane = get_angle(oplane_pos, fplane_pos)\n",
    "        angle_to_obase = get_angle(obase_pos, fplane_pos)\n",
    "        rel_angle_oplane = (angle_to_oplane - fplane_angle) % 360\n",
    "        rel_angle_obase = (angle_to_obase - fplane_angle) % 360\n",
    "\n",
    "        # --------------- FORWARDS ---------------\n",
    "        if action == 0: \n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "\n",
    "         # --------------- SHOOT ---------------\n",
    "        elif action == 1:\n",
    "            self.bullets.append(Bullet(fplane_pos[0], fplane_pos[1], fplane_angle, self.bullet_speed, self.friendly, oteam))\n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "        \n",
    "        # --------------- TURN RIGHT ---------------\n",
    "        elif action == 2:\n",
    "            fplane.rotate(-self.step_turn)\n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "\n",
    "        # ---------------- TURN LEFT ----------------\n",
    "        elif action == 3:\n",
    "            fplane.rotate(self.step_turn)\n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "\n",
    "        # ---------------- TURN TO OPLANE ----------------\n",
    "        elif action == 4:\n",
    "            if math.fabs(rel_angle_oplane) < self.step_turn: # within step_turn of base\n",
    "                fplane.set_heading(angle_to_oplane)\n",
    "\n",
    "            elif math.fabs(rel_angle_oplane) > 360 - self.step_turn: # within step_turn of base\n",
    "                fplane.set_heading(angle_to_oplane)\n",
    "\n",
    "            elif math.fabs(rel_angle_oplane) < 180: # turn right\n",
    "                fplane.rotate(-self.step_turn)\n",
    "\n",
    "            else: # turn left\n",
    "                fplane.rotate(self.step_turn)\n",
    "                \n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "\n",
    "        # ---------------- TURN TO OBASE ----------------\n",
    "        elif action == 5:\n",
    "            if math.fabs(rel_angle_obase) < self.step_turn: # within step_turn of base\n",
    "                fplane.set_heading(angle_to_obase)\n",
    "\n",
    "            elif math.fabs(rel_angle_obase) > 360 - self.step_turn: # within step_turn of base\n",
    "                fplane.set_heading(angle_to_obase)\n",
    "\n",
    "            elif math.fabs(rel_angle_obase) < 180: # turn right\n",
    "                fplane.rotate(-self.step_turn)\n",
    "\n",
    "            else: # turn left\n",
    "                fplane.rotate(self.step_turn)\n",
    "\n",
    "            fplane.forward(self.speed, self.time_step)\n",
    "        \n",
    "        # ---------------- GIVE REWARDS IF CLOSER (DIST OR ANGLE) ----------------\n",
    "        new_fplane_pos = fplane.get_pos()\n",
    "        new_fplane_angle = fplane.heading\n",
    "        new_oplane_pos = oplane.get_pos()\n",
    "        new_obase_pos = obase.get_pos()\n",
    "\n",
    "        new_dist_oplane = dist(new_oplane_pos, new_fplane_pos)\n",
    "        new_dist_obase = dist(new_obase_pos, new_fplane_pos)\n",
    "\n",
    "        new_angle_to_oplane = get_angle(new_oplane_pos, new_fplane_pos)\n",
    "        new_angle_to_obase = get_angle(new_obase_pos, new_fplane_pos)\n",
    "        new_rel_angle_oplane = (new_angle_to_oplane - new_fplane_angle) % 360\n",
    "        new_rel_angle_obase = (new_angle_to_obase - new_fplane_angle) % 360\n",
    "\n",
    "        if new_dist_oplane < dist_oplane: # If got closer to enemy plane\n",
    "            reward += self.closer_to_plane_reward\n",
    "\n",
    "        if new_dist_obase < dist_obase: # If got closer to enemy base\n",
    "            reward += self.closer_to_base_reward\n",
    "\n",
    "        if math.fabs(new_rel_angle_oplane) < math.fabs(rel_angle_oplane): # If aiming closer to enemy plane\n",
    "            reward += self.turn_to_plane_reward\n",
    "\n",
    "        if math.fabs(new_rel_angle_obase) < math.fabs(rel_angle_obase): # If aiming closer to enemy base\n",
    "            reward += self.turn_to_base_reward\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def draw_shot(self, hit, friendly_pos, target_pos, team):\n",
    "        color = (0, 0, 0)\n",
    "        color = (255, 0, 0) if team == 'red' else (0, 0, 255)\n",
    "        if not hit: target_pos = (target_pos[0] + (random.random() * 2 - 1) * 100, target_pos[1] + (random.random() * 2 - 1) * 100)\n",
    "        self.shot_history.append((hit, friendly_pos, target_pos, color))\n",
    "    \n",
    "    def winner_screen(self):\n",
    "        if self.show:\n",
    "            font = pygame.font.Font('freesansbold.ttf', 32)\n",
    "            if self.winner != 'none':\n",
    "                text = font.render(f\"THE WINNER IS {self.winner.upper()}\", True, (0, 0, 0))\n",
    "                textRect = text.get_rect()\n",
    "                textRect.center = (DISP_WIDTH//2, DISP_HEIGHT//2)\n",
    "            else:\n",
    "                text = font.render(f\"THE GAME IS A TIE\", True, (0, 0, 0))\n",
    "                textRect = text.get_rect()\n",
    "                textRect.center = (DISP_WIDTH//2, DISP_HEIGHT//2)\n",
    "            self.display.blit(text, textRect)\n",
    "\n",
    "    def show_wins(self):\n",
    "        print(\"Wins by red:\", self.team['red']['wins'])\n",
    "        print(\"Wins by blue:\", self.team['blue']['wins'])\n",
    "        print(\"Tied games:\", self.ties)\n",
    "\n",
    "    def render(self):\n",
    "        if self.show: # Just to ensure it won't render if self.show == False\n",
    "            for event in pygame.event.get():\n",
    "                # Check for KEYDOWN event\n",
    "                if event.type == KEYDOWN:\n",
    "                    # If the Esc key is pressed, then exit the main loop\n",
    "                    if event.key == K_ESCAPE:\n",
    "                        pygame.quit()\n",
    "                        sys.exit()\n",
    "                        return\n",
    "                # Check for QUIT event. If QUIT, then set running to false.\n",
    "                elif event.type == QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "                    return\n",
    "                    \n",
    "            # Fill background\n",
    "            self.display.fill(WHITE)\n",
    "\n",
    "            # Draw bullets\n",
    "            for bullet in self.bullets:\n",
    "                bullet.draw(self.display)\n",
    "                    \n",
    "            # Draw bases\n",
    "            self.team['red']['base'].draw(self.display)\n",
    "            self.team['blue']['base'].draw(self.display)\n",
    "\n",
    "            # Draw planes\n",
    "            for plane in self.team['red']['planes']:\n",
    "                plane.update()\n",
    "                plane.draw(self.display)\n",
    "            for plane in self.team['blue']['planes']:\n",
    "                plane.update()\n",
    "                plane.draw(self.display)\n",
    "\n",
    "            # Winner Screen\n",
    "            if self.done:\n",
    "                font = pygame.font.Font('freesansbold.ttf', 32)\n",
    "                if self.winner != 'none':\n",
    "                    text = font.render(f\"THE WINNER IS {self.winner.upper()}\", True, (0, 0, 0))\n",
    "                    textRect = text.get_rect()\n",
    "                    textRect.center = (DISP_WIDTH//2, DISP_HEIGHT//2)\n",
    "                else:\n",
    "                    text = font.render(f\"THE GAME IS A TIE\", True, (0, 0, 0))\n",
    "                    textRect = text.get_rect()\n",
    "                    textRect.center = (DISP_WIDTH//2, DISP_HEIGHT//2)\n",
    "                self.display.blit(text, textRect)\n",
    "                pygame.display.update()\n",
    "                pygame.time.wait(3000)\n",
    "                pygame.quit()\n",
    "                return\n",
    "            \n",
    "            pygame.display.update()\n",
    "            FPS.tick(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Hyperparameter Tuning ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==1.10.2+cu113 torchvision==0.11.3+cu113 torchaudio===0.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "%pip install stable-baselines3[extra] optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "if not os.path.exists(OPT_DIR):\n",
    "    os.makedirs(OPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ppo(trial): \n",
    "    return {\n",
    "        'n_steps':trial.suggest_int('n_steps', 2048, 8192),\n",
    "        'gamma':trial.suggest_loguniform('gamma', 0.8, 0.9999),\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
    "        'clip_range':trial.suggest_uniform('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda':trial.suggest_uniform('gae_lambda', 0.8, 0.99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        model_params = optimize_ppo(trial) \n",
    "\n",
    "        # Create environment \n",
    "        env = BattleEnvironment()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        model = PPO('MlpPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        model.learn(total_timesteps=400000)\n",
    "\n",
    "        # Evaluate model \n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-26 16:19:21,952]\u001b[0m A new study created in memory with name: no-name-f83b9314-4689-48e0-8fa3-274453f9450d\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4786`, after every 74 untruncated mini-batches, there will be a truncated mini-batch of size 50\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=4786 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 16:29:44,321]\u001b[0m Trial 0 finished with value: 102.8 and parameters: {'n_steps': 4786, 'gamma': 0.971550686333748, 'learning_rate': 1.5097327356026387e-05, 'clip_range': 0.34148511908874346, 'gae_lambda': 0.842651713694691}. Best is trial 0 with value: 102.8.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2778`, after every 43 untruncated mini-batches, there will be a truncated mini-batch of size 26\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2778 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 16:40:05,190]\u001b[0m Trial 1 finished with value: 287.6 and parameters: {'n_steps': 2778, 'gamma': 0.8183254905816083, 'learning_rate': 4.953550427923822e-05, 'clip_range': 0.3742727667812864, 'gae_lambda': 0.8694423217102631}. Best is trial 1 with value: 287.6.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2779`, after every 43 untruncated mini-batches, there will be a truncated mini-batch of size 27\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2779 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 16:50:36,647]\u001b[0m Trial 2 finished with value: 103.4 and parameters: {'n_steps': 2779, 'gamma': 0.8710912947644858, 'learning_rate': 8.739206302970651e-05, 'clip_range': 0.12124563337900854, 'gae_lambda': 0.8761767700706475}. Best is trial 1 with value: 287.6.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4753`, after every 74 untruncated mini-batches, there will be a truncated mini-batch of size 17\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=4753 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 17:01:20,930]\u001b[0m Trial 3 finished with value: 103.2 and parameters: {'n_steps': 4753, 'gamma': 0.8711884600497829, 'learning_rate': 3.9458807376614526e-05, 'clip_range': 0.19705954164123052, 'gae_lambda': 0.9475950498958993}. Best is trial 1 with value: 287.6.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4164`, after every 65 untruncated mini-batches, there will be a truncated mini-batch of size 4\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=4164 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 17:11:54,199]\u001b[0m Trial 4 finished with value: 105.4 and parameters: {'n_steps': 4164, 'gamma': 0.880212688552024, 'learning_rate': 8.972008407167931e-05, 'clip_range': 0.10687520737081546, 'gae_lambda': 0.888087481607243}. Best is trial 1 with value: 287.6.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5922`, after every 92 untruncated mini-batches, there will be a truncated mini-batch of size 34\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=5922 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 17:22:10,226]\u001b[0m Trial 5 finished with value: 89.4 and parameters: {'n_steps': 5922, 'gamma': 0.9687047613486433, 'learning_rate': 5.8729777821097935e-05, 'clip_range': 0.3386091309201468, 'gae_lambda': 0.9495858994609168}. Best is trial 1 with value: 287.6.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 7382`, after every 115 untruncated mini-batches, there will be a truncated mini-batch of size 22\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=7382 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 17:32:21,458]\u001b[0m Trial 6 finished with value: 122.6 and parameters: {'n_steps': 7382, 'gamma': 0.9402605930106438, 'learning_rate': 9.865635701659469e-05, 'clip_range': 0.24082833296649778, 'gae_lambda': 0.8221528788848892}. Best is trial 1 with value: 287.6.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3653`, after every 57 untruncated mini-batches, there will be a truncated mini-batch of size 5\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3653 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 17:42:08,305]\u001b[0m Trial 7 finished with value: 278.4 and parameters: {'n_steps': 3653, 'gamma': 0.8419501804204907, 'learning_rate': 7.313415275508126e-05, 'clip_range': 0.30941093937227004, 'gae_lambda': 0.8757350250512804}. Best is trial 1 with value: 287.6.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2616`, after every 40 untruncated mini-batches, there will be a truncated mini-batch of size 56\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2616 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 17:51:43,328]\u001b[0m Trial 8 finished with value: 280.6 and parameters: {'n_steps': 2616, 'gamma': 0.9501790362921417, 'learning_rate': 7.949301618655148e-05, 'clip_range': 0.2190832404328784, 'gae_lambda': 0.8714895743234}. Best is trial 1 with value: 287.6.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5585`, after every 87 untruncated mini-batches, there will be a truncated mini-batch of size 17\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=5585 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 18:01:27,579]\u001b[0m Trial 9 finished with value: 285.4 and parameters: {'n_steps': 5585, 'gamma': 0.839376567788934, 'learning_rate': 1.2917180899313909e-05, 'clip_range': 0.14231003136870962, 'gae_lambda': 0.8061241035173424}. Best is trial 1 with value: 287.6.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 7932`, after every 123 untruncated mini-batches, there will be a truncated mini-batch of size 60\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=7932 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 18:11:11,290]\u001b[0m Trial 10 finished with value: 287.4 and parameters: {'n_steps': 7932, 'gamma': 0.8038727520737573, 'learning_rate': 2.4709440394524237e-05, 'clip_range': 0.39749759556560704, 'gae_lambda': 0.9210707786959407}. Best is trial 1 with value: 287.6.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8111`, after every 126 untruncated mini-batches, there will be a truncated mini-batch of size 47\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=8111 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 18:20:45,625]\u001b[0m Trial 11 finished with value: 104.8 and parameters: {'n_steps': 8111, 'gamma': 0.8011004905553847, 'learning_rate': 2.3559700904378088e-05, 'clip_range': 0.3959456630652065, 'gae_lambda': 0.9211332711230128}. Best is trial 1 with value: 287.6.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6354`, after every 99 untruncated mini-batches, there will be a truncated mini-batch of size 18\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6354 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 18:30:47,112]\u001b[0m Trial 12 finished with value: 307.0 and parameters: {'n_steps': 6354, 'gamma': 0.8241762204693597, 'learning_rate': 3.3158699298144134e-05, 'clip_range': 0.3974894031940379, 'gae_lambda': 0.9119210197111699}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6983`, after every 109 untruncated mini-batches, there will be a truncated mini-batch of size 7\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6983 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 18:40:32,991]\u001b[0m Trial 13 finished with value: 111.6 and parameters: {'n_steps': 6983, 'gamma': 0.8301071033743885, 'learning_rate': 4.302573460543227e-05, 'clip_range': 0.3214519635552193, 'gae_lambda': 0.916256539152556}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6269`, after every 97 untruncated mini-batches, there will be a truncated mini-batch of size 61\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6269 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 18:50:00,962]\u001b[0m Trial 14 finished with value: 101.4 and parameters: {'n_steps': 6269, 'gamma': 0.8248672085436648, 'learning_rate': 3.064693005366742e-05, 'clip_range': 0.28253179574776005, 'gae_lambda': 0.8485505647119551}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2094`, after every 32 untruncated mini-batches, there will be a truncated mini-batch of size 46\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2094 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 19:00:27,234]\u001b[0m Trial 15 finished with value: 291.0 and parameters: {'n_steps': 2094, 'gamma': 0.9142608161175854, 'learning_rate': 4.9646953813801336e-05, 'clip_range': 0.3701297698138494, 'gae_lambda': 0.9699432716537013}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "\u001b[32m[I 2022-06-26 19:10:49,505]\u001b[0m Trial 16 finished with value: 98.0 and parameters: {'n_steps': 6656, 'gamma': 0.8992887453480608, 'learning_rate': 1.7446066204923273e-05, 'clip_range': 0.36264648314339076, 'gae_lambda': 0.9850100070422819}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2149`, after every 33 untruncated mini-batches, there will be a truncated mini-batch of size 37\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2149 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 19:20:52,489]\u001b[0m Trial 17 finished with value: 110.4 and parameters: {'n_steps': 2149, 'gamma': 0.9250942724075909, 'learning_rate': 3.3213579968257205e-05, 'clip_range': 0.27875518616091854, 'gae_lambda': 0.9792063679716803}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3800`, after every 59 untruncated mini-batches, there will be a truncated mini-batch of size 24\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3800 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 19:30:45,988]\u001b[0m Trial 18 finished with value: 111.8 and parameters: {'n_steps': 3800, 'gamma': 0.8998096708909676, 'learning_rate': 5.603402046298758e-05, 'clip_range': 0.36357831124253365, 'gae_lambda': 0.9573295835234029}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5411`, after every 84 untruncated mini-batches, there will be a truncated mini-batch of size 35\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=5411 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 19:40:48,240]\u001b[0m Trial 19 finished with value: 95.6 and parameters: {'n_steps': 5411, 'gamma': 0.9946063587856537, 'learning_rate': 2.3108679971093696e-05, 'clip_range': 0.2970858548194535, 'gae_lambda': 0.9076328238052344}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4272`, after every 66 untruncated mini-batches, there will be a truncated mini-batch of size 48\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=4272 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 19:51:57,672]\u001b[0m Trial 20 finished with value: 104.0 and parameters: {'n_steps': 4272, 'gamma': 0.9143269039367854, 'learning_rate': 3.015148409115432e-05, 'clip_range': 0.17007436432379103, 'gae_lambda': 0.9646388723941814}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3130`, after every 48 untruncated mini-batches, there will be a truncated mini-batch of size 58\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3130 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 20:06:25,033]\u001b[0m Trial 21 finished with value: 105.0 and parameters: {'n_steps': 3130, 'gamma': 0.855244626168814, 'learning_rate': 4.712500529033001e-05, 'clip_range': 0.3755562391233336, 'gae_lambda': 0.8521021387515161}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2490`, after every 38 untruncated mini-batches, there will be a truncated mini-batch of size 58\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2490 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 20:20:05,011]\u001b[0m Trial 22 finished with value: 280.0 and parameters: {'n_steps': 2490, 'gamma': 0.8167936542509701, 'learning_rate': 5.522192571985261e-05, 'clip_range': 0.37701801703404453, 'gae_lambda': 0.8958300781027383}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3244`, after every 50 untruncated mini-batches, there will be a truncated mini-batch of size 44\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3244 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 20:33:43,652]\u001b[0m Trial 23 finished with value: 100.4 and parameters: {'n_steps': 3244, 'gamma': 0.8557352614529602, 'learning_rate': 3.7287406746026363e-05, 'clip_range': 0.33518978957725554, 'gae_lambda': 0.9342009180105741}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2141`, after every 33 untruncated mini-batches, there will be a truncated mini-batch of size 29\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2141 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-26 20:47:59,558]\u001b[0m Trial 24 finished with value: 93.0 and parameters: {'n_steps': 2141, 'gamma': 0.8194509027145852, 'learning_rate': 6.4124667986393e-05, 'clip_range': 0.39876008874893154, 'gae_lambda': 0.8634409960276019}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3469`, after every 54 untruncated mini-batches, there will be a truncated mini-batch of size 13\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3469 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 21:01:49,518]\u001b[0m Trial 25 finished with value: 298.8 and parameters: {'n_steps': 3469, 'gamma': 0.8867738799751667, 'learning_rate': 5.158510478106449e-05, 'clip_range': 0.358259884757668, 'gae_lambda': 0.8964363894791995}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3329`, after every 52 untruncated mini-batches, there will be a truncated mini-batch of size 1\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=3329 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 21:01:59,110]\u001b[0m Trial 26 finished with value: -1000.0 and parameters: {'n_steps': 3329, 'gamma': 0.9159567912682718, 'learning_rate': 6.884460972555864e-05, 'clip_range': 0.26453495097319635, 'gae_lambda': 0.9368562547757922}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4968`, after every 77 untruncated mini-batches, there will be a truncated mini-batch of size 40\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=4968 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 21:16:04,855]\u001b[0m Trial 27 finished with value: 96.6 and parameters: {'n_steps': 4968, 'gamma': 0.8854773619524544, 'learning_rate': 3.560177410000703e-05, 'clip_range': 0.35024851774536064, 'gae_lambda': 0.8971365040764765}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6305`, after every 98 untruncated mini-batches, there will be a truncated mini-batch of size 33\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6305 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 21:29:02,283]\u001b[0m Trial 28 finished with value: 106.6 and parameters: {'n_steps': 6305, 'gamma': 0.9369752153366099, 'learning_rate': 2.7388384625206463e-05, 'clip_range': 0.31710701848764844, 'gae_lambda': 0.9734354975035047}. Best is trial 12 with value: 307.0.\u001b[0m\n",
      "E:\\Anaconda\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4341`, after every 67 untruncated mini-batches, there will be a truncated mini-batch of size 53\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=4341 and n_envs=1)\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-06-26 21:40:05,372]\u001b[0m Trial 29 finished with value: 113.0 and parameters: {'n_steps': 4341, 'gamma': 0.966837776661233, 'learning_rate': 1.6661167613049246e-05, 'clip_range': 0.34615891374719276, 'gae_lambda': 0.9358937054090397}. Best is trial 12 with value: 307.0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------- Load the Best Model ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params\n",
    "study.best_trial\n",
    "best_model = 'trial_12_best_model.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(os.path.join(OPT_DIR, best_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Callback ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Train the Model ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BattleEnvironment()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model_params = study.best_params\n",
    "model_params['n_steps'] = (model_params['n_steps'] // 64) * 64 # set to a factor of 64\n",
    "model = PPO('MlpPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/PPO_34\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.1     |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 541      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 6336     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 89.9        |\n",
      "|    ep_rew_mean          | 56.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 400         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 12672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019567937 |\n",
      "|    clip_fraction        | 0.00775     |\n",
      "|    clip_range           | 0.397       |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.00188     |\n",
      "|    learning_rate        | 3.32e-05    |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 3.89e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 89          |\n",
      "|    ep_rew_mean          | 93.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 19008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025961878 |\n",
      "|    clip_fraction        | 0.0762      |\n",
      "|    clip_range           | 0.397       |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.00819     |\n",
      "|    learning_rate        | 3.32e-05    |\n",
      "|    loss                 | 20.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    value_loss           | 3.07e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 93.2       |\n",
      "|    ep_rew_mean          | 80.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 363        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 69         |\n",
      "|    total_timesteps      | 25344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03468804 |\n",
      "|    clip_fraction        | 0.063      |\n",
      "|    clip_range           | 0.397      |\n",
      "|    entropy_loss         | -1.64      |\n",
      "|    explained_variance   | 0.0112     |\n",
      "|    learning_rate        | 3.32e-05   |\n",
      "|    loss                 | 5.2e+03    |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    value_loss           | 3.64e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 95.7       |\n",
      "|    ep_rew_mean          | 82.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 88         |\n",
      "|    total_timesteps      | 31680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05839457 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.397      |\n",
      "|    entropy_loss         | -1.4       |\n",
      "|    explained_variance   | 0.0086     |\n",
      "|    learning_rate        | 3.32e-05   |\n",
      "|    loss                 | 12.7       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0307    |\n",
      "|    value_loss           | 1.47e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 93.6       |\n",
      "|    ep_rew_mean          | 140        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 108        |\n",
      "|    total_timesteps      | 38016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06852565 |\n",
      "|    clip_fraction        | 0.0903     |\n",
      "|    clip_range           | 0.397      |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 0.0122     |\n",
      "|    learning_rate        | 3.32e-05   |\n",
      "|    loss                 | 84.8       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0283    |\n",
      "|    value_loss           | 1.11e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 96.7        |\n",
      "|    ep_rew_mean          | 123         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 348         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 127         |\n",
      "|    total_timesteps      | 44352       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037367053 |\n",
      "|    clip_fraction        | 0.038       |\n",
      "|    clip_range           | 0.397       |\n",
      "|    entropy_loss         | -0.835      |\n",
      "|    explained_variance   | 0.00561     |\n",
      "|    learning_rate        | 3.32e-05    |\n",
      "|    loss                 | 2.48e+03    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 2.43e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 94.9        |\n",
      "|    ep_rew_mean          | 153         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 146         |\n",
      "|    total_timesteps      | 50688       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021671154 |\n",
      "|    clip_fraction        | 0.0509      |\n",
      "|    clip_range           | 0.397       |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.0083      |\n",
      "|    learning_rate        | 3.32e-05    |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 1.44e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 97.5        |\n",
      "|    ep_rew_mean          | 133         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 57024       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012465707 |\n",
      "|    clip_fraction        | 0.0164      |\n",
      "|    clip_range           | 0.397       |\n",
      "|    entropy_loss         | -0.422      |\n",
      "|    explained_variance   | 0.00822     |\n",
      "|    learning_rate        | 3.32e-05    |\n",
      "|    loss                 | 9.34        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00758    |\n",
      "|    value_loss           | 1.46e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 96.4         |\n",
      "|    ep_rew_mean          | 145          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 345          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 183          |\n",
      "|    total_timesteps      | 63360        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074782204 |\n",
      "|    clip_fraction        | 0.0128       |\n",
      "|    clip_range           | 0.397        |\n",
      "|    entropy_loss         | -0.28        |\n",
      "|    explained_variance   | 0.0084       |\n",
      "|    learning_rate        | 3.32e-05     |\n",
      "|    loss                 | 7.82         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00613     |\n",
      "|    value_loss           | 375          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 95.5         |\n",
      "|    ep_rew_mean          | 158          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 344          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 202          |\n",
      "|    total_timesteps      | 69696        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051663397 |\n",
      "|    clip_fraction        | 0.00229      |\n",
      "|    clip_range           | 0.397        |\n",
      "|    entropy_loss         | -0.217       |\n",
      "|    explained_variance   | 0.0123       |\n",
      "|    learning_rate        | 3.32e-05     |\n",
      "|    loss                 | 12.2         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00337     |\n",
      "|    value_loss           | 1.75e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 95.4         |\n",
      "|    ep_rew_mean          | 162          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 343          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 221          |\n",
      "|    total_timesteps      | 76032        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030369053 |\n",
      "|    clip_fraction        | 0.00368      |\n",
      "|    clip_range           | 0.397        |\n",
      "|    entropy_loss         | -0.141       |\n",
      "|    explained_variance   | 0.0141       |\n",
      "|    learning_rate        | 3.32e-05     |\n",
      "|    loss                 | 7.68e+03     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0028      |\n",
      "|    value_loss           | 1.37e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 96.5        |\n",
      "|    ep_rew_mean          | 158         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 240         |\n",
      "|    total_timesteps      | 82368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001044239 |\n",
      "|    clip_fraction        | 0.000126    |\n",
      "|    clip_range           | 0.397       |\n",
      "|    entropy_loss         | -0.111      |\n",
      "|    explained_variance   | 0.0126      |\n",
      "|    learning_rate        | 3.32e-05    |\n",
      "|    loss                 | 7.88        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00126    |\n",
      "|    value_loss           | 2.1e+03     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 92           |\n",
      "|    ep_rew_mean          | 200          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 340          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 260          |\n",
      "|    total_timesteps      | 88704        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004018603 |\n",
      "|    clip_fraction        | 7.89e-05     |\n",
      "|    clip_range           | 0.397        |\n",
      "|    entropy_loss         | -0.0892      |\n",
      "|    explained_variance   | 0.018        |\n",
      "|    learning_rate        | 3.32e-05     |\n",
      "|    loss                 | 1.54e+04     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    value_loss           | 1.09e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 95.7          |\n",
      "|    ep_rew_mean          | 164           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 340           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 279           |\n",
      "|    total_timesteps      | 95040         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019501662 |\n",
      "|    clip_fraction        | 6.31e-05      |\n",
      "|    clip_range           | 0.397         |\n",
      "|    entropy_loss         | -0.0881       |\n",
      "|    explained_variance   | 0.0178        |\n",
      "|    learning_rate        | 3.32e-05      |\n",
      "|    loss                 | 9.54          |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.000366     |\n",
      "|    value_loss           | 2.85e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 94.1         |\n",
      "|    ep_rew_mean          | 186          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 339          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 298          |\n",
      "|    total_timesteps      | 101376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005126698 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.397        |\n",
      "|    entropy_loss         | -0.0633      |\n",
      "|    explained_variance   | 0.0229       |\n",
      "|    learning_rate        | 3.32e-05     |\n",
      "|    loss                 | 2.44e+03     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000644    |\n",
      "|    value_loss           | 1.43e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 96.5         |\n",
      "|    ep_rew_mean          | 155          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 338          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 317          |\n",
      "|    total_timesteps      | 107712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002694327 |\n",
      "|    clip_fraction        | 7.89e-05     |\n",
      "|    clip_range           | 0.397        |\n",
      "|    entropy_loss         | -0.0513      |\n",
      "|    explained_variance   | 0.0227       |\n",
      "|    learning_rate        | 3.32e-05     |\n",
      "|    loss                 | 11.7         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000413    |\n",
      "|    value_loss           | 2.48e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 94.6          |\n",
      "|    ep_rew_mean          | 171           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 338           |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 336           |\n",
      "|    total_timesteps      | 114048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.9561385e-05 |\n",
      "|    clip_fraction        | 4.73e-05      |\n",
      "|    clip_range           | 0.397         |\n",
      "|    entropy_loss         | -0.0465       |\n",
      "|    explained_variance   | 0.0215        |\n",
      "|    learning_rate        | 3.32e-05      |\n",
      "|    loss                 | 8.95          |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.000258     |\n",
      "|    value_loss           | 1.09e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 94.7         |\n",
      "|    ep_rew_mean          | 171          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 338          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 355          |\n",
      "|    total_timesteps      | 120384       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.377572e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.397        |\n",
      "|    entropy_loss         | -0.042       |\n",
      "|    explained_variance   | 0.0255       |\n",
      "|    learning_rate        | 3.32e-05     |\n",
      "|    loss                 | 10.9         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000322    |\n",
      "|    value_loss           | 1.71e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 97.4          |\n",
      "|    ep_rew_mean          | 144           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 338           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 374           |\n",
      "|    total_timesteps      | 126720        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.5426688e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.397         |\n",
      "|    entropy_loss         | -0.0414       |\n",
      "|    explained_variance   | 0.0258        |\n",
      "|    learning_rate        | 3.32e-05      |\n",
      "|    loss                 | 790           |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.000103     |\n",
      "|    value_loss           | 1.79e+03      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 97.4         |\n",
      "|    ep_rew_mean          | 143          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 338          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 393          |\n",
      "|    total_timesteps      | 133056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002479873 |\n",
      "|    clip_fraction        | 0.000316     |\n",
      "|    clip_range           | 0.397        |\n",
      "|    entropy_loss         | -0.0388      |\n",
      "|    explained_variance   | 0.0329       |\n",
      "|    learning_rate        | 3.32e-05     |\n",
      "|    loss                 | 6.83         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000857    |\n",
      "|    value_loss           | 375          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 92            |\n",
      "|    ep_rew_mean          | 203           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 338           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 412           |\n",
      "|    total_timesteps      | 139392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.5938354e-05 |\n",
      "|    clip_fraction        | 7.89e-05      |\n",
      "|    clip_range           | 0.397         |\n",
      "|    entropy_loss         | -0.0311       |\n",
      "|    explained_variance   | 0.0226        |\n",
      "|    learning_rate        | 3.32e-05      |\n",
      "|    loss                 | 802           |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.000567     |\n",
      "|    value_loss           | 1.44e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 96.6          |\n",
      "|    ep_rew_mean          | 157           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 337           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 431           |\n",
      "|    total_timesteps      | 145728        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8743005e-05 |\n",
      "|    clip_fraction        | 3.16e-05      |\n",
      "|    clip_range           | 0.397         |\n",
      "|    entropy_loss         | -0.0342       |\n",
      "|    explained_variance   | 0.0194        |\n",
      "|    learning_rate        | 3.32e-05      |\n",
      "|    loss                 | 7.63e+03      |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.000158     |\n",
      "|    value_loss           | 2.29e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 96.5          |\n",
      "|    ep_rew_mean          | 158           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 337           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 450           |\n",
      "|    total_timesteps      | 152064        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034893793 |\n",
      "|    clip_fraction        | 0.000189      |\n",
      "|    clip_range           | 0.397         |\n",
      "|    entropy_loss         | -0.0283       |\n",
      "|    explained_variance   | 0.0177        |\n",
      "|    learning_rate        | 3.32e-05      |\n",
      "|    loss                 | 8.81          |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.000633     |\n",
      "|    value_loss           | 733           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 95.8         |\n",
      "|    ep_rew_mean          | 165          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 337          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 470          |\n",
      "|    total_timesteps      | 158400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.481747e-05 |\n",
      "|    clip_fraction        | 7.89e-05     |\n",
      "|    clip_range           | 0.397        |\n",
      "|    entropy_loss         | -0.0258      |\n",
      "|    explained_variance   | 0.0205       |\n",
      "|    learning_rate        | 3.32e-05     |\n",
      "|    loss                 | 8.53         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000187    |\n",
      "|    value_loss           | 1.78e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 97.6          |\n",
      "|    ep_rew_mean          | 145           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 336           |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 489           |\n",
      "|    total_timesteps      | 164736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3325664e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.397         |\n",
      "|    entropy_loss         | -0.0217       |\n",
      "|    explained_variance   | 0.0198        |\n",
      "|    learning_rate        | 3.32e-05      |\n",
      "|    loss                 | 7.59          |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -9.52e-05     |\n",
      "|    value_loss           | 1.09e+03      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.load(os.path.join(OPT_DIR, best_model))\n",
    "model.learn(total_timesteps=4000000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- Evaluation ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BattleEnvironment()\n",
    "env = Monitor(train_env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(train_env, 4, channels_order='last')\n",
    "model = PPO.load(f\"{OPT_DIR}{best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "for episode in range(episodes): # Evaluates the model n times\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    while not env.done:\n",
    "        action, _states = model.predict(state)\n",
    "        n_state, red_reward, done, info = env.step(action)\n",
    "        score += red_reward\n",
    "    print('Episode:{} Score:{}'.format(episode+1, score))\n",
    "env.show_wins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------- Render One Episode ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.show = True\n",
    "state = env.reset()\n",
    "score = 0\n",
    "while not train_env.done:\n",
    "    env.render()\n",
    "    action, _states = model.predict(state)\n",
    "    n_state, red_reward, done, info = env.step(action)\n",
    "    score += red_reward\n",
    "print('Score:{}'.format(score))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy_of_CAE_RL_battle_simulation_mar_25_2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b5efcd3efc71ef29576cdfc4a5c5091a22a4d39f277c681ebc64abd29d3aec9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
